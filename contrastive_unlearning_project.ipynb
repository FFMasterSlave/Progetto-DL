{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Constrastive Unelarning with CIFAR-10/SVHN and ResNet*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook there is all the project for the ***Deep Learning and Applied Artificial Intelligence*** course.\n",
    "\n",
    "I have replicated the pipeline and the experiments of the article *\"A Contrastive Approach to Machine Unlearning\"*, and i will also try to enhance the performance of this approach by implementing it in **Hyperbolic Spaces**, which is a representation space that is gaining recent interest in the **Computer Vision field**. \n",
    "\n",
    "Since the article only tells the architectures used without their configuration, i took the initiative and built ResNets specifically for CIFAR-10 and SVHN datasets.\n",
    "\n",
    "The notebook is divided in chapters where you will find short explenation on what is happening and why.\n",
    "\n",
    "You may find operations on code of previous chapters, but i tried to make the whole notebook organic so that something is instatiated only when is necessary and not before.\n",
    "\n",
    "Since i used this narrative-driven style of notebook, it is suggested to run cells in order, otherwise some later part of the codes may not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ray tqdm plotly matplotlib sklearn hypll tabulate numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch and torchvision-related imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.amp\n",
    "import torchvision \n",
    "\n",
    "# Ray Tune for hyperparameter tuning\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "# Visualization and utility imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Typing for function signatures\n",
    "from typing import Optional\n",
    "\n",
    "# Plotting and manifold learning\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "# Hypll and Geoopt for hyperbolic space operations\n",
    "from hypll import nn as hnn\n",
    "from hypll.tensors import ManifoldTensor, TangentTensor\n",
    "from hypll.manifolds.poincare_ball import Curvature, PoincareBall\n",
    "from hypll.optim import RiemannianAdam, RiemannianSGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotly renderer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed because Plotly has some problems with Visual Studio Code interactive plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.renderers.default = 'notebook_connected'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image show function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    \"\"\"\n",
    "    Takes an image (or a batch of images) and show it using plt.\n",
    "    \n",
    "    Args:\n",
    "    - img: single image or batch of images to show\n",
    "    \"\"\"\n",
    "\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings extraction and plotting functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *create_plot* function uses **t-SNE** as a dimensionality reduction method to allow the data to be plotted in 3D. \n",
    "\n",
    "**WARNING**: it works well, but it is quite slow and runs on CPU, so plot the graphs only if you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(encoder, loader, hyperbolic='', manifold=None, device='cuda'):\n",
    "    \"\"\"\n",
    "    Extracts the embeddings and the relative labels of a DataLoader\n",
    "    \n",
    "    Args:\n",
    "    - encoder: ResNet encoder\n",
    "    - loader: the DataLoader of the testset\n",
    "\n",
    "    Returns:\n",
    "    - np.concatenate(all_features): all the features concatenated to be shown\n",
    "    - np.concatenate(all_labels): all the labels concatenated to be shown\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            # The projection is needed ony if the whole model is hyperbolic\n",
    "            if hyperbolic in ['complete-ResNet', 'academic-ResNet']:\n",
    "                # move the inputs to the manifold\n",
    "                tangents = TangentTensor(data=inputs, man_dim=1, manifold=manifold)\n",
    "                inputs = manifold.expmap(tangents)\n",
    "\n",
    "            features = encoder(inputs)\n",
    "\n",
    "            if hyperbolic in ['complete-ResNet', 'hybrid-ResNet', 'academic-ResNet']:\n",
    "                features = features.tensor\n",
    "                #labels = labels.tensor\n",
    "    \n",
    "            all_features.append(features.cpu().numpy())\n",
    "            all_labels.append(labels.numpy())\n",
    "\n",
    "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
    "\n",
    "\n",
    "\n",
    "def create_plot(features, labels, class_names, dimension=3, convexhull=False):\n",
    "    \"\"\"\n",
    "    Creates a plot of the distribution of features in the space.\n",
    "    Plots the convex hull of each class, showing class names on hover.\n",
    "    \n",
    "    Args:\n",
    "    - features: the embeddings of the testset\n",
    "    - labels: the labels of the embeddings\n",
    "    - class_names: list of class names to display on hover (should correspond to label numbers)\n",
    "    - dimension: dimension of the plot (2 or 3)\n",
    "    - convexhull: if True, create a Convex Hull around the classes samples (only for 2D)\n",
    "    \"\"\"\n",
    "\n",
    "    # Reduce to the specified dimension using t-SNE\n",
    "    tsne = TSNE(n_components=dimension, random_state=42)\n",
    "    features_reduced = tsne.fit_transform(features)\n",
    "\n",
    "    # Get unique labels\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    # Create a Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot each class separately to enable toggling in legend\n",
    "    for label in unique_labels:\n",
    "        # Get points corresponding to the current class\n",
    "        class_points = features_reduced[labels == label]\n",
    "\n",
    "        # Set up a unique legend group for each class to toggle convex hull visibility\n",
    "        legendgroup = f\"group_{label}\"\n",
    "\n",
    "        if dimension == 3:\n",
    "            # 3D Scatter plot for 3D data\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=class_points[:, 0],\n",
    "                y=class_points[:, 1],\n",
    "                z=class_points[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(size=3, opacity=0.5),\n",
    "                name=class_names[label],\n",
    "                hoverinfo='text',\n",
    "                text=[class_names[label]] * len(class_points),\n",
    "                legendgroup=legendgroup\n",
    "            ))\n",
    "\n",
    "            # Add convex hull if enabled and enough points\n",
    "            if convexhull and len(class_points) >= 4:\n",
    "                hull = ConvexHull(class_points)\n",
    "                hull_vertices = class_points[hull.vertices]\n",
    "                fig.add_trace(go.Mesh3d(\n",
    "                    x=hull_vertices[:, 0],\n",
    "                    y=hull_vertices[:, 1],\n",
    "                    z=hull_vertices[:, 2],\n",
    "                    opacity=0.5,\n",
    "                    color='rgba(255, 0, 0, 0.2)',\n",
    "                    showlegend=False,\n",
    "                    legendgroup=legendgroup,\n",
    "                    hoverinfo='skip'\n",
    "                ))\n",
    "\n",
    "        elif dimension == 2:\n",
    "            # 2D Scatter plot for 2D data\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=class_points[:, 0],\n",
    "                y=class_points[:, 1],\n",
    "                mode='markers',\n",
    "                marker=dict(size=5, opacity=0.5),\n",
    "                name=class_names[label],\n",
    "                hoverinfo='text',\n",
    "                text=[class_names[label]] * len(class_points),\n",
    "                legendgroup=legendgroup\n",
    "            ))\n",
    "\n",
    "            # Add convex hull if enabled and enough points\n",
    "            if convexhull and len(class_points) >= 3:\n",
    "                hull = ConvexHull(class_points)\n",
    "                hull_vertices = class_points[hull.vertices]\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=hull_vertices[:, 0],\n",
    "                    y=hull_vertices[:, 1],\n",
    "                    fill='toself',\n",
    "                    opacity=0.5,\n",
    "                    fillcolor='rgba(255, 0, 0, 0.2)',\n",
    "                    line=dict(color='rgba(255, 0, 0, 0.2)'),\n",
    "                    showlegend=False,\n",
    "                    legendgroup=legendgroup,\n",
    "                    hoverinfo='skip'\n",
    "                ))\n",
    "\n",
    "    # Update layout for better visualization and interactivity\n",
    "    if dimension == 3:\n",
    "        fig.update_layout(\n",
    "            scene=dict(\n",
    "                xaxis_title='Component 1',\n",
    "                yaxis_title='Component 2',\n",
    "                zaxis_title='Component 3'\n",
    "            ),\n",
    "            title=\"3D t-SNE of Extracted Features with Class Toggles\",\n",
    "            template=\"plotly_dark\",\n",
    "            legend=dict(\n",
    "                title=\"Classes\",\n",
    "                itemsizing='constant',\n",
    "                x=0.8\n",
    "            )\n",
    "        )\n",
    "    elif dimension == 2:\n",
    "        fig.update_layout(\n",
    "            xaxis_title='Component 1',\n",
    "            yaxis_title='Component 2',\n",
    "            title=\"2D t-SNE of Extracted Features with Class Toggles\",\n",
    "            template=\"plotly_dark\",\n",
    "            legend=dict(\n",
    "                title=\"Classes\",\n",
    "                itemsizing='constant',\n",
    "                x=1.0\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Show interactive plot\n",
    "    fig.show()\n",
    "    #fig.write_html('./plot.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(train_loss_history, val_loss_history, train_acc_history, val_acc_history):\n",
    "    \"\"\"\n",
    "    This function plots the training and validation loss/accuracy history and includes test performance.\n",
    "\n",
    "    Args:\n",
    "    - train_loss_history: List of training loss values per epoch\n",
    "    - val_loss_history: List of validation loss values per epoch\n",
    "    - train_acc_history: List of training accuracy values per epoch\n",
    "    - val_acc_history: List of validation accuracy values per epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a figure for loss\n",
    "    fig_loss = go.Figure()\n",
    "\n",
    "    fig_loss.add_trace(go.Scatter(x=list(range(len(train_loss_history))), y=train_loss_history,\n",
    "                        mode='lines+markers',\n",
    "                        name='Train Loss'))\n",
    "    fig_loss.add_trace(go.Scatter(x=list(range(len(val_loss_history))), y=val_loss_history,\n",
    "                        mode='lines+markers',\n",
    "                        name='Validation Loss'))\n",
    "\n",
    "    # Update layout for loss plot\n",
    "    fig_loss.update_layout(title='Loss over Epochs',\n",
    "                       xaxis_title='Epoch',\n",
    "                       yaxis_title='Loss',\n",
    "                       template=\"plotly_dark\")\n",
    "\n",
    "    # Show loss figure\n",
    "    fig_loss.show()\n",
    "\n",
    "    # Create a figure for accuracy\n",
    "    fig_acc = go.Figure()\n",
    "\n",
    "    fig_acc.add_trace(go.Scatter(x=list(range(len(train_acc_history))), y=train_acc_history,\n",
    "                        mode='lines+markers',\n",
    "                        name='Train Accuracy'))\n",
    "    fig_acc.add_trace(go.Scatter(x=list(range(len(val_acc_history))), y=val_acc_history,\n",
    "                        mode='lines+markers',\n",
    "                        name='Validation Accuracy'))\n",
    "\n",
    "    # Update layout for accuracy plot\n",
    "    fig_acc.update_layout(title='Accuracy over Epochs',\n",
    "                       xaxis_title='Epoch',\n",
    "                       yaxis_title='Accuracy (%)',\n",
    "                       template=\"plotly_dark\")\n",
    "\n",
    "    # Show accuracy figure\n",
    "    fig_acc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopper class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that Pytorch does not have a built in EralyStopping, so i implemented one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5, min_delta=0.1):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section will be instatiated all the non-destructive data that is needed through the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global DATASET\n",
    "global SCENARIO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the constants that will be used throughout the course of the notebook.\n",
    "\n",
    "They define the experiments configuration, so it is advised to change them only here at the start of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1. Choose the dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET = 'CIFAR-10'\n",
    "DATASET = 'SVHN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2. Choose the unlearning method***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCENARIO = 'single-class'\n",
    "SCENARIO = 'random-sample'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***3. Choose the model type***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL_NAME = 'ResNet18'\n",
    "MODEL_NAME = 'ResNet34'\n",
    "#MODEL_NAME = 'ResNet50'\n",
    "#MODEL_NAME = 'ResNet101'\n",
    "#MODEL_NAME = 'ResNet152'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***4. Choose the optimizer***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: It is not advised to use the *Adam* optimizer for the project, since it makes too-strong parameter updates and causes the later unlearning procedure to forget also the samples we want to mantain.\n",
    "\n",
    "That's why i ended up to use only SGD.\n",
    "\n",
    "I still gave the opportunity to test it, just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIMIZER_NAME = 'adam' # NOT ADVISED TO USE\n",
    "OPTIMIZER_NAME = 'sgd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***5. Classification loss***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITERION = nn.CrossEntropyLoss(label_smoothing=0.1) #0.0447361 #0.022574"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***6. Training epochs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***7. Checkpoint names***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will create a *data* and *checkpoint* directories to store the datasets and the individual checkpoints.\n",
    "\n",
    "Only the *original* and *retrain* model checkpoints will be saved.\n",
    "\n",
    "Contrastive unlearning is performed exclusively at execution time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EUCL_ORIGINAL_CKPT_NAME = f'{DATASET}_eucl_original_{MODEL_NAME}_{OPTIMIZER_NAME}_{SCENARIO}_ckpt.pth'\n",
    "EUCL_RETRAIN_CKPT_NAME = f'{DATASET}_eucl_retrain_{MODEL_NAME}_{OPTIMIZER_NAME}_{SCENARIO}_ckpt.pth'\n",
    "HYPBL_ORIGINAL_CKPT_NAME = f'{DATASET}_hypbl_original_{MODEL_NAME}_{OPTIMIZER_NAME}_{SCENARIO}_ckpt.pth'\n",
    "HYPBL_RETRAIN_CKPT_NAME = f'{DATASET}_hypbl_retrain_{MODEL_NAME}_{OPTIMIZER_NAME}_{SCENARIO}_ckpt.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***8. Unlearning hyperparameters***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values below are found with **Ray Tune** hyperparameter tuning.\n",
    "\n",
    "Since the authors of the original article didn't specify anything about these (except for omega), i had to try some combinations and found these to be the best.\n",
    "\n",
    "However, since the nature of the unlearning procedure is heavly dependent on random sampling data, these fixed parameters will not guarantee results that are always precise to the last digit.\n",
    "\n",
    "Playing with these will modifiy two aspects of unlearning: convergence speed and knowledge keeping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TEMPEARTURE** -> modulates how aggressively the unlearning samples are pushed from positive samples and attracted to negative ones      (tested values: [0.1, 0.8])\n",
    "- **OMEGA** -> determines the times each unlearning sample batch is compared with different sets of remaining samples                       (tested values: [2, 4, 6])\n",
    "- **REGULARIZER_CE** -> balance how much we want to mantain knowledge of the remaining samples                                              (tested values: [0.3, 1.0])\n",
    "- **REGULARIZER_UL** -> balance how efficently we want to forget the unwanted samples                                                       (tested values: [0.3, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# euclidean\n",
    "TEMPERATURE = 0.3      #0.21\n",
    "OMEGA = 4               #6\n",
    "REGULARIZER_CE = 0.18   #0.31\n",
    "REGULARIZER_UL = 0.71   #0.48\n",
    "\n",
    "# hyperbolic\n",
    "HYPBL_TEMPERATURE = 0.69\n",
    "HYPBL_OMEGA = 4\n",
    "HYPBL_REGULARIZER_CE = 0.23\n",
    "HYPBL_REGULARIZER_UL = 0.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***9. Choose the hyperbolic architecture***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPBL_ARCHITECTURE = 'complete-ResNet'\n",
    "HYPBL_ARCHITECTURE = 'hybrid-ResNet'\n",
    "#HYPBL_ARCHITECTURE = 'academic-ResNet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***10. Hyperbolic model and curvature definition***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the curvature i chose the value *-1* because usually a hyperbolic space has a negative constant value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURVATURE = Curvature(value=-1.0)\n",
    "MANIFOLD = PoincareBall(c=CURVATURE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Can i fix the data splits for reproducibility?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally for testing i tought about fixing a seed to reduce randomness of the results, but i fiure out this is not possible.\n",
    "\n",
    "If you fix all the data splits, then the unlearning procedure won't work anymore since is based on random sampling.\n",
    "\n",
    "That's also why the results from the article are, and will always be, different from mine.\n",
    "\n",
    "Despite this, the experiment still has a meaning: experiment a novel approach to Machine Unlearning (Contrastive Unlearning) with a novel approach to data representation (Hyperbolic spaces) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the main article steps, no data augmentations will be used.\n",
    "\n",
    "The only thing that changes are the normalization values for the chosen dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "validation_split = 0.2\n",
    "\n",
    "\n",
    "if DATASET == 'CIFAR-10':\n",
    "\n",
    "    preprocess = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=preprocess)\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=preprocess)\n",
    "\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')    \n",
    "\n",
    "if DATASET == 'SVHN':\n",
    "\n",
    "    preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.4377, 0.4438, 0.4728), (0.198, 0.201, 0.197)),\n",
    "])\n",
    "\n",
    "    trainset = torchvision.datasets.SVHN(root='./data', split='train', download=True, transform=preprocess)\n",
    "    testset = torchvision.datasets.SVHN(root='./data', split='test', download=True, transform=preprocess)\n",
    "\n",
    "    classes = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')\n",
    "\n",
    "\n",
    "# Determine the size of the validation set\n",
    "train_size = int((1 - validation_split) * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "\n",
    "# Split the training dataset into training and validation sets\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [train_size, val_size])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Contrastive Unlearning Pipeline on Euclidean Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this macro-section i will be replicating the setup of the main article.\n",
    "\n",
    "As mentioned at the start of the notebook, the authors didn't include some important specs of the pipeline so i had to improvise while keeping general consistency for the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I gave the possibility to test the whole project with different versions of the ResNet architecture.\n",
    "\n",
    "The available models are: ResNet 18/34/50/101/152\n",
    "\n",
    "After some testing, i found that ResNet 34 offers the best precision-over-time efficency so i ended up focusing on it. \n",
    "\n",
    "It is safe to say that if it works for this version, the deeper architectures should have even better accuracy results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ResNet in PyTorch.\n",
    "\n",
    "Heavly inspired by:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.dropout = nn.Dropout(p=0.3)  # Add dropout layer\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.dropout(out)  # Apply dropout after the last batch norm\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
    "        self.dropout = nn.Dropout(p=0.3)  # Add dropout layer\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out = self.dropout(out)  # Apply dropout after the last batch norm\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.dropout = nn.Dropout(p=0.3)  # Add Dropout layer\n",
    "        \n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dropout(out) # Add Dropout layer\n",
    "        return out\n",
    "    \n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=10):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.5) # Add Dropout layer\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    encoder = ResNetEncoder(BasicBlock, [2, 2, 2, 2])\n",
    "    classification = ClassificationHead(512, num_classes=len(classes))\n",
    "    return encoder, classification\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    encoder = ResNetEncoder(BasicBlock, [3, 4, 6, 3])\n",
    "    classification = ClassificationHead(512, num_classes=len(classes))\n",
    "    return encoder, classification\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    encoder = ResNetEncoder(Bottleneck, [3, 4, 6, 3])\n",
    "    classification = ClassificationHead(2048, num_classes=len(classes))\n",
    "    return encoder, classification\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    encoder = ResNetEncoder(Bottleneck, [3, 4, 23, 3])\n",
    "    classification = ClassificationHead(2048, num_classes=len(classes))\n",
    "    return encoder, classification\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    encoder = ResNetEncoder(Bottleneck, [3, 8, 36, 3])\n",
    "    classification = ClassificationHead(2048, num_classes=len(classes))\n",
    "    return encoder, classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO NOT UNCOMMENT THESE: Hyperparameter tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used **Ray Tune** to find the most promising hyperparameters for the training phase.\n",
    "\n",
    "The hyperparameter tested where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Architecture** --> ResNet18, ResNet34\n",
    "- **Optimizer** --> SGD, Adam\n",
    "- **Learning rate** --> [1e-4, 1e-1]\n",
    "- **Momentum** --> [0.5, 0.9] only for SGD\n",
    "- **Weight decay** --> [1e-6, 1e-2]\n",
    "- **T_max** --> 100, 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I collected the best hyperparameter for training with SGD and Adam independetly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Put large objects in Ray's object store for efficient memory management\n",
    "large_trainloader = ray.put(trainloader)\n",
    "large_valloader = ray.put(valloader)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def train_and_evaluate(config):\n",
    "\n",
    "    # Retrieve objects stored via Ray\n",
    "    trainloader = ray.get(large_trainloader)\n",
    "    valloader = ray.get(large_valloader)\n",
    "\n",
    "    scaler = torch.amp.grad_scaler.GradScaler()\n",
    "\n",
    "    # Model architecture setup\n",
    "    if config['arch'] == 'ResNet18':\n",
    "        encoder, classifier = ResNet18()\n",
    "    elif config['arch'] == 'ResNet34':\n",
    "        encoder, classifier = ResNet34()\n",
    "\n",
    "    # Move the models to the correct device (e.g., GPU)\n",
    "    encoder = encoder.to(device)\n",
    "    classifier = classifier.to(device)\n",
    "\n",
    "    # Loss and optimizer setup\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=config['label_smoothing'])\n",
    "\n",
    "    if config['opt'] == 'sgd':\n",
    "        optimizer = optim.SGD(\n",
    "            list(encoder.parameters()) + list(classifier.parameters()),\n",
    "            lr=config[\"lr\"],\n",
    "            momentum=config[\"momentum\"],\n",
    "            weight_decay=config[\"weight_decay\"]\n",
    "        )\n",
    "    elif config['opt'] == 'adam':\n",
    "        optimizer = optim.Adam(\n",
    "            list(encoder.parameters()) + list(classifier.parameters()),\n",
    "            lr=config[\"lr\"],\n",
    "            weight_decay=config[\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config[\"T_max\"])\n",
    "\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        encoder.train()\n",
    "        classifier.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Training loop\n",
    "        for inputs, targets in trainloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast_mode.autocast(device_type=device, dtype=torch.float16):\n",
    "                features = encoder(inputs)\n",
    "                outputs = classifier(features)\n",
    "                \n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # Evaluation phase\n",
    "        encoder.eval()\n",
    "        classifier.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valloader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                features = encoder(inputs)\n",
    "                outputs = classifier(features)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # Calculate test loss and accuracy\n",
    "        val_loss /= len(valloader)\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "\n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Report metrics to Ray Tune\n",
    "        ray.train.report(dict(loss=val_loss, accuracy=val_acc, from_epoch=epoch))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Custom trial directory name creator\n",
    "def custom_trial_name(trial):\n",
    "    return f\"trial_{trial.trial_id}\"\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "search_space = {\n",
    "    \"arch\": tune.grid_search(['ResNet18', 'ResNet34']),\n",
    "    \"opt\": tune.grid_search(['sgd', 'adam']),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    \"momentum\": tune.uniform(0.5, 0.9),\n",
    "    \"weight_decay\": tune.loguniform(1e-6, 1e-2),\n",
    "    \"T_max\": tune.choice([100, 200]),\n",
    "    \"label_smoothing\": tune.uniform(0.0, 0.2),\n",
    "    \"num_epochs\": tune.grid_search([15]) \n",
    "}\n",
    "\n",
    "\n",
    "# Define the scheduler for Ray Tune\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=15,  # Maximum number of epochs for any trial\n",
    "    grace_period=7,  # Minimum number of epochs before a trial can be stopped\n",
    "    reduction_factor=2  # How aggressively trials are stopped (larger means more aggressive)\n",
    ")\n",
    "\n",
    "# Launch Ray Tune hyperparameter tuning\n",
    "analysis = tune.run(\n",
    "    train_and_evaluate,\n",
    "    resources_per_trial={\"cpu\": 20, \"gpu\": 1},\n",
    "    config=search_space,\n",
    "    num_samples=5,\n",
    "    scheduler=scheduler,\n",
    "    trial_dirname_creator=custom_trial_name  # Use custom trial name\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is set to parallelize if multiple GPUs are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_NAME == 'ResNet18':\n",
    "    encoder, classifier = ResNet18()\n",
    "elif MODEL_NAME == 'ResNet34':\n",
    "    encoder, classifier = ResNet34()\n",
    "elif MODEL_NAME == 'ResNet50':\n",
    "    encoder, classifier = ResNet50()\n",
    "elif MODEL_NAME == 'ResNet101':\n",
    "    encoder, classifier = ResNet101()\n",
    "elif MODEL_NAME == 'ResNet152':\n",
    "    encoder, classifier = ResNet152()\n",
    "\n",
    "\n",
    "if device == 'cuda':\n",
    "    encoder = torch.nn.DataParallel(encoder).to(device)\n",
    "    classifier = torch.nn.DataParallel(classifier).to(device)\n",
    "    cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Optimizer and scheduler configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the optimizers and scheduler with the best hyperparameters found via **Ray Tune** for each dataset.\n",
    "\n",
    "Again, it is not advised to use *Adam* optimizer, since the paramters updates are too strong and will compromise the Constrastive Unlearning procedure.\n",
    "\n",
    "I still left the option to do so though, just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST CONFIGURATION ADAM\n",
    "if OPTIMIZER_NAME == 'adam':\n",
    "    optimizer = optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=0.000641388, weight_decay=0.567643)\n",
    "\n",
    "# BEST CONFIGURATION SGD\n",
    "elif OPTIMIZER_NAME == 'sgd':\n",
    "    optimizer = optim.SGD(list(encoder.parameters()) + list(classifier.parameters()), lr=0.015254, momentum=0.582486, weight_decay=0.00366885)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Validate and Test functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encapsulated the whole training pipeline into functions for better understanding of what happens.\n",
    "\n",
    "Also, they are convinient for keeping track of the changes made on the hyperbolic variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, classifier, loader, criterion, optimizer, scaler, hyperbolic='', manifold=None, device='cuda'):\n",
    "    \"\"\"\n",
    "    The training pipeline. It takes as input all the necessary components to perform model training\n",
    "    \n",
    "    Args:\n",
    "    - encoder: ResNet encoder\n",
    "    - classifier: ResNet classifier\n",
    "    - loader: the DataLoader of the trainset\n",
    "    - criterion: the loss function (CrossEntropyLoss)\n",
    "    - optimizer: do the updates to the parameters (SGD or Adam)\n",
    "    - scaler: it is needed to performe training with mixed precision (16 float point, instead of 32)\n",
    "    - hyperbolic: if True it means that the hyperbolic ResNet is running, and so some adjustment are needed to the embeddings  \n",
    "\n",
    "    Returns\n",
    "    - average_loss: the average of the loss computed by dividing the loss accumulation by the lenght of the loader of the train set\n",
    "    - acc: the accuracy of the model on the train set\n",
    "    \"\"\"\n",
    "\n",
    "    progress_bar_train = tqdm(enumerate(loader), total=len(loader))\n",
    "\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in progress_bar_train:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # The projection is needed ony if the whole model is hyperbolic\n",
    "        if hyperbolic in ['complete-ResNet', 'academic-ResNet']:\n",
    "            # move the inputs to the manifold\n",
    "            tangents = TangentTensor(data=inputs, man_dim=1, manifold=manifold)\n",
    "            inputs = manifold.expmap(tangents)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.amp.autocast_mode.autocast(device_type=device, dtype=torch.float16):\n",
    "            features = encoder(inputs)\n",
    "            outputs = classifier(features)\n",
    "\n",
    "            # Indipendently of the hyperbolic model, we need to extract the tensor  \n",
    "            if hyperbolic in ['complete-ResNet', 'hybrid-ResNet', 'academic-ResNet']:\n",
    "                # Needed for the loss computation\n",
    "                outputs = outputs.tensor # This gives the underlying PyTorch tensor\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "    \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar_train.set_description('TRAIN | Loss: %.3f | Acc: %.3f%% (%d/%d)' % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    average_loss = train_loss/len(loader)\n",
    "    acc = 100.*correct/total\n",
    "\n",
    "    return average_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation function save a checkpoint of the model when a new best accuracy is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch, encoder, classifier, loader, criterion, best_acc, hyperbolic='', manifold=None, ckpt_name='', device='cuda'):\n",
    "    \"\"\"\n",
    "    The validation pipeline. It takes as input all the necessary components to perform model validation\n",
    "    \n",
    "    Args:\n",
    "    - epoch: needed for finding which epoch produced the best accuracy result of the model\n",
    "    - encoder: ResNet encoder\n",
    "    - classifier: ResNet classifier\n",
    "    - loader: the DataLoader of the validationset\n",
    "    - criterion: the loss function (CrossEntropyLoss)\n",
    "    - best_acc: it keeps trace of the best accuracy so far (needed to save the model checkpoint)\n",
    "    - hyperbolic: if True it means that the hyperbolic ResNet is running, and so some adjustment are needed to the embeddings\n",
    "    - ckpt_name: the name of the checkpoint\n",
    "\n",
    "    Returns:\n",
    "    - average_loss: the average of the loss computed by dividing the loss accumulation by the lenght of the loader of the validation set\n",
    "    - best_acc: the best accuracy of the model on the validation set\n",
    "    \"\"\"\n",
    "\n",
    "    progress_bar_val = tqdm(enumerate(loader), total=len(loader))\n",
    "\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in progress_bar_val:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # The projection is needed ony if the whole model is hyperbolic\n",
    "            if hyperbolic in ['complete-ResNet', 'academic-ResNet']:\n",
    "                # move the inputs to the manifold\n",
    "                tangents = TangentTensor(data=inputs, man_dim=1, manifold=manifold)\n",
    "                inputs = manifold.expmap(tangents)\n",
    "\n",
    "            features = encoder(inputs)\n",
    "            outputs = classifier(features)\n",
    "            \n",
    "            # Indipendently of the hyperbolic model, we need to extract the tensor  \n",
    "            if hyperbolic in ['complete-ResNet', 'hybrid-ResNet', 'academic-ResNet']:\n",
    "                # Needed for the loss computation\n",
    "                outputs = outputs.tensor # This gives the underlying PyTorch tensor\n",
    "                \n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            acc = 100.*correct/total\n",
    "\n",
    "            progress_bar_val.set_description('VAL | Loss: %.3f | Acc: %.3f%% (%d/%d)' % (val_loss/(batch_idx+1), acc, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    average_loss = val_loss / len(loader)\n",
    "\n",
    "    if acc > best_acc:\n",
    "        print('Saving checkpoint..')\n",
    "        state = {\n",
    "            'encoder': encoder.state_dict(),\n",
    "            'classifier': classifier.state_dict(),\n",
    "            'loss': average_loss,\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/{}'.format(ckpt_name))\n",
    "        best_acc = acc\n",
    "\n",
    "    return average_loss, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder, classifier, loader, criterion, hyperbolic='', manifold=None, device='cuda'):\n",
    "    \"\"\"\n",
    "    The test pipeline. It takes as input all the necessary components to perform model testing\n",
    "    \n",
    "    Args:\n",
    "    - encoder: ResNet encoder\n",
    "    - classifier: ResNet classifier\n",
    "    - loader: the DataLoader of the testeset\n",
    "    - criterion: the loss function (CrossEntropyLoss)\n",
    "    - hyperbolic: if True it means that the hyperbolic ResNet is running, and so some adjustment are needed to the embeddings\n",
    "\n",
    "    Returns:\n",
    "    - average_loss: the average of the loss computed by dividing the loss accumulation by the lenght of the loader of the test set\n",
    "    - best_acc: the best accuracy of the model on the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    progress_bar_test = tqdm(enumerate(loader), total=len(loader))\n",
    "\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in progress_bar_test:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # The projection is needed ony if the whole model is hyperbolic\n",
    "            if hyperbolic in ['complete-ResNet', 'academic-ResNet']:\n",
    "                # move the inputs to the manifold\n",
    "                tangents = TangentTensor(data=inputs, man_dim=1, manifold=manifold)\n",
    "                inputs = manifold.expmap(tangents)\n",
    "\n",
    "            features = encoder(inputs)\n",
    "            outputs = classifier(features)\n",
    "            \n",
    "            # Indipendently of the hyperbolic model, we need to extract the tensor  \n",
    "            if hyperbolic in ['complete-ResNet', 'hybrid-ResNet', 'academic-ResNet']:\n",
    "                # Needed for the loss computation\n",
    "                outputs = outputs.tensor # This gives the underlying PyTorch tensor\n",
    "                \n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            acc = 100.*correct/total\n",
    "\n",
    "            progress_bar_test.set_description('TEST | Loss: %.3f | Acc: %.3f%% (%d/%d)' % (test_loss/(batch_idx+1), acc, correct, total))\n",
    "\n",
    "    average_loss = test_loss / len(loader)\n",
    "\n",
    "    return average_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the article don't use any data augmentations, the model tend to overfit.\n",
    "\n",
    "Said so, in order to replicate the results and test them in hyperbolic space, i have to keep this configuration.\n",
    "\n",
    "The EarlyStopper class is used to stop the training when the validation accuracy stop performing meaningfull improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to track the loss and accuracy\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "val_acc = 0 \n",
    "\n",
    "scaler = torch.amp.grad_scaler.GradScaler()\n",
    "\n",
    "early_stopper = EarlyStopper()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "\n",
    "    average_train_loss, train_acc = train(encoder=encoder, \n",
    "                                          classifier=classifier, \n",
    "                                          loader=trainloader, \n",
    "                                          criterion=CRITERION, \n",
    "                                          optimizer=optimizer, \n",
    "                                          scaler=scaler, \n",
    "                                          hyperbolic='', \n",
    "                                          manifold=None, \n",
    "                                          device=device)\n",
    "    \n",
    "    train_loss_history.append(average_train_loss)\n",
    "    train_acc_history.append(train_acc)\n",
    "    \n",
    "    average_val_loss, val_acc = validate(epoch=epoch, \n",
    "                                         encoder=encoder, \n",
    "                                         classifier=classifier, \n",
    "                                         loader=valloader, \n",
    "                                         criterion=CRITERION, \n",
    "                                         best_acc=val_acc, \n",
    "                                         hyperbolic='', \n",
    "                                         manifold=None, \n",
    "                                         ckpt_name=EUCL_ORIGINAL_CKPT_NAME, \n",
    "                                         device=device)\n",
    "    \n",
    "    val_loss_history.append(average_val_loss)\n",
    "    val_acc_history.append(val_acc)\n",
    "\n",
    "    if early_stopper.early_stop(average_val_loss):  \n",
    "        print('\\nEarly stopper activated')           \n",
    "        break\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss and accuracy over the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(train_loss_history, val_loss_history, train_acc_history, val_acc_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load best checkpoint.\n",
    "\n",
    "It is needed since there is no guarantee that the latest epochs produced the most accurate parameters.\n",
    "\n",
    "Also comes in handy for quick testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Resuming from checkpoint...')\n",
    "\n",
    "assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "checkpoint = torch.load('./checkpoint/{}'.format(EUCL_ORIGINAL_CKPT_NAME))\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "classifier.load_state_dict(checkpoint['classifier'])\n",
    "\n",
    "print('Checkpoint loaded!')\n",
    "print(f'\\nValidation average loss: {checkpoint['loss']:.3f}')\n",
    "print(f'Validation set accuracy: {checkpoint['acc']:.3f}%')\n",
    "print(f'From epoch: {checkpoint['epoch']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the model accuracy over some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "encoder.eval()\n",
    "classifier.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = encoder(images)\n",
    "    outputs = classifier(features)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images[0:16]))\n",
    "\n",
    "# Prepare rows with GroundTruth and Predicted labels\n",
    "rows = [[\"GroundTruth\", *[classes[labels[j]] for j in range(16)]],\n",
    "        [\"Predicted\", *[classes[predicted[j]] for j in range(16)]]]\n",
    "\n",
    "# Display the table\n",
    "print(tabulate(rows, headers=[\"\"] + [f\"Image {i+1}\" for i in range(16)], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model overall performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = test(encoder=encoder, \n",
    "                           classifier=classifier, \n",
    "                           loader=testloader, \n",
    "                           criterion=CRITERION, \n",
    "                           hyperbolic='', \n",
    "                           manifold=None, \n",
    "                           device=device)\n",
    "\n",
    "print(f'\\nTest Set - Loss: {test_loss:.3f}, Accuracy: {test_acc:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose which class samples to display by toggling the class names.\n",
    "\n",
    "**WARNING**: As said in the *Configuration* section, Plotly has some problems with Visual Studio Code. To prevent strange behaviour by the plots, it is advised to delete the cell plot output first if you need to run again the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the test set, and plot them\n",
    "features, labels = extract_features(encoder, testloader, hyperbolic='', manifold=None, device=device)\n",
    "create_plot(features, labels, classes, dimension=2, convexhull=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainset, Valset and Testset splitted into: sample to unlearn, sample to mantain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a crucial step which is needed also in the unlearning procedure.\n",
    "\n",
    "Everyone of the three original sets is splitted into the samples we want to unlearn and the ones we want to mantain.\n",
    "\n",
    "**ATTENTION**: in the case of *random-sample* scenario, the *\"remaining_testset\"* is the whole original test set. This because, in this scenario the goal is to preserve the accuracy of the models even minus these random samples.\n",
    "\n",
    "**WARNING**: in the case of *random-sample* scenario, running this cell multiple times WILL COMPROMISE the final result. This is not an error; it's expected behavior, so it's not recommended to proceed with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_samples(dataset, class_to_remove, samples_to_remove):\n",
    "    \"\"\"\n",
    "    Split samples based on scenario (class or random) into two datasets: \n",
    "    unlearning and remaining.\n",
    "    \n",
    "    Args:\n",
    "    - dataset: the dataset to filter (e.g., CIFAR-10, SVHN)\n",
    "    - class_to_remove: when the scenario is 'single-class' it indicates the class we want to unlearn\n",
    "    - samples_to_remove: when the scenario is 'random-sample' it indicates the amount of random samples we want to unlearn\n",
    "    \n",
    "    Returns:\n",
    "    - unlearning_dataset: Dataset containing the selected samples for unlearning\n",
    "    - remaining_dataset: Dataset with the remaining samples\n",
    "    \"\"\"\n",
    "\n",
    "    if SCENARIO == 'single-class':\n",
    "        indices_to_remove = [i for i, (img, label) in enumerate(dataset) if label == class_to_remove]\n",
    "        indices_to_keep = [i for i, (img, label) in enumerate(dataset) if label != class_to_remove]\n",
    "\n",
    "    elif SCENARIO == 'random-sample':\n",
    "        total_indices = list(range(len(dataset)))\n",
    "        indices_to_remove = random.sample(total_indices, samples_to_remove)\n",
    "        indices_to_keep = list(set(total_indices) - set(indices_to_remove))\n",
    "    \n",
    "    # Create the unlearning and remaining datasets\n",
    "    unlearning_dataset = torch.utils.data.Subset(dataset, indices_to_remove)\n",
    "    remaining_dataset = torch.utils.data.Subset(dataset, indices_to_keep)\n",
    "\n",
    "    return unlearning_dataset, remaining_dataset, indices_to_remove\n",
    "            \n",
    "if SCENARIO == 'single-class': \n",
    "    # here the unlearned class is removed from every set\n",
    "    class_to_remove = 5\n",
    "    unlearning_trainset, remaining_trainset, _ = remove_samples(trainset, class_to_remove=class_to_remove, samples_to_remove=None)\n",
    "    _, remaining_valset, _ = remove_samples(valset, class_to_remove=class_to_remove, samples_to_remove=None) \n",
    "    unlearning_testset , remaining_testset, unlearning_test_indices = remove_samples(testset, class_to_remove=class_to_remove, samples_to_remove=None) \n",
    "\n",
    "    # Next to each magazine there is a label indicating which part it will be used in\n",
    "    remaining_trainloader = torch.utils.data.DataLoader(remaining_trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True) #retrain #unlearning\n",
    "    unlearning_trainloader = torch.utils.data.DataLoader(unlearning_trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True) #unlearning #experiments\n",
    "    remaining_valloader = torch.utils.data.DataLoader(remaining_valset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True) #retrain #validation\n",
    "    remaining_testloader = torch.utils.data.DataLoader(remaining_testset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True) #retest #experiments\n",
    "    unlearning_testloader = torch.utils.data.DataLoader(unlearning_testset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True) #unlearning #experiments\n",
    "\n",
    "elif SCENARIO == 'random-sample':\n",
    "    samples_to_remove = 500\n",
    "    unlearning_trainset, remaining_trainset, unlearning_train_indices = remove_samples(trainset, class_to_remove=None, samples_to_remove=samples_to_remove)\n",
    "    remaining_testset = testset\n",
    "\n",
    "    remaining_trainloader = torch.utils.data.DataLoader(remaining_trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True) #retrain #unlearning\n",
    "    unlearning_trainloader = torch.utils.data.DataLoader(unlearning_trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True) #unlearning #experiments\n",
    "    remaining_valloader = valloader #retrain\n",
    "    remaining_testloader = testloader   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, another instantiation of the model will be trained without the samples that we want to forget (from a single class or with random sampling).\n",
    "\n",
    "Most of the functions are reused from the previous chapters, so refer to them if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_NAME == 'ResNet18':\n",
    "    retrain_encoder, retrain_classifier = ResNet18()\n",
    "elif MODEL_NAME == 'ResNet34':\n",
    "    retrain_encoder, retrain_classifier = ResNet34()\n",
    "elif MODEL_NAME == 'ResNet50':\n",
    "    retrain_encoder, retrain_classifier = ResNet50()\n",
    "elif MODEL_NAME == 'ResNet101':\n",
    "    retrain_encoder, retrain_classifier = ResNet101()\n",
    "elif MODEL_NAME == 'ResNet152':\n",
    "    retrain_encoder, retrain_classifier = ResNet152()\n",
    "\n",
    "\n",
    "if device == 'cuda':\n",
    "    retrain_encoder = torch.nn.DataParallel(retrain_encoder).to(device)\n",
    "    retrain_classifier = torch.nn.DataParallel(retrain_classifier).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and scheduler configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST CONFIGURATION ADAM\n",
    "if OPTIMIZER_NAME == 'adam':\n",
    "    retrain_optimizer = optim.Adam(list(retrain_encoder.parameters()) + list(retrain_classifier.parameters()), lr=0.000641388, weight_decay=0.567643)\t\n",
    "\n",
    "# BEST CONFIGURATION SGD\n",
    "elif OPTIMIZER_NAME == 'sgd':\n",
    "    retrain_optimizer = optim.SGD(list(retrain_encoder.parameters()) + list(retrain_classifier.parameters()), lr=0.015254, momentum=0.582486, weight_decay=0.00366885) \n",
    "    \n",
    "retrain_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(retrain_optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to track the loss and accuracy\n",
    "retrain_train_loss_history = []\n",
    "retrain_val_loss_history = []\n",
    "retrain_train_acc_history = []\n",
    "retrain_val_acc_history = []\n",
    "\n",
    "retrain_val_acc = 0 \n",
    "\n",
    "scaler = torch.amp.grad_scaler.GradScaler()\n",
    "\n",
    "early_stopper = EarlyStopper()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "\n",
    "    retrain_average_train_loss, retrain_train_acc = train(encoder=retrain_encoder, \n",
    "                                                          classifier=retrain_classifier, \n",
    "                                                          loader=remaining_trainloader, \n",
    "                                                          criterion=CRITERION, \n",
    "                                                          optimizer=retrain_optimizer, \n",
    "                                                          scaler=scaler, \n",
    "                                                          hyperbolic='', \n",
    "                                                          manifold=None, \n",
    "                                                          device=device)\n",
    "    \n",
    "    retrain_train_loss_history.append(retrain_average_train_loss)\n",
    "    retrain_train_acc_history.append(retrain_train_acc)\n",
    "\n",
    "    retrain_average_val_loss, retrain_val_acc = validate(epoch=epoch, \n",
    "                                                         encoder=retrain_encoder, \n",
    "                                                         classifier=retrain_classifier, \n",
    "                                                         loader=remaining_valloader, \n",
    "                                                         criterion=CRITERION, \n",
    "                                                         best_acc=retrain_val_acc, \n",
    "                                                         hyperbolic='', \n",
    "                                                         manifold=None, \n",
    "                                                         ckpt_name=EUCL_RETRAIN_CKPT_NAME, \n",
    "                                                         device=device)\n",
    "    \n",
    "    retrain_val_loss_history.append(retrain_average_val_loss)\n",
    "    retrain_val_acc_history.append(retrain_val_acc)\n",
    "\n",
    "    if early_stopper.early_stop(retrain_average_val_loss):\n",
    "        print('\\nEarly stopper activated') \n",
    "        break\n",
    "\n",
    "    retrain_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(retrain_train_loss_history, retrain_val_loss_history, retrain_train_acc_history, retrain_val_acc_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load best checkpoint.\n",
    "\n",
    "It is needed since there is no guarantee that the latest epochs produced the most accurate parameters.\n",
    "\n",
    "Also comes in handy for quick testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Resuming from checkpoint...')\n",
    "\n",
    "assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "checkpoint = torch.load('./checkpoint/{}'.format(EUCL_RETRAIN_CKPT_NAME))\n",
    "retrain_encoder.load_state_dict(checkpoint['encoder'])\n",
    "retrain_classifier.load_state_dict(checkpoint['classifier'])\n",
    "\n",
    "print('Checkpoint loaded!')\n",
    "print(f'\\nValidation average loss: {checkpoint['loss']:.3f}')\n",
    "print(f'Validation set accuracy: {checkpoint['acc']:.3f}%')\n",
    "print(f'From epoch: {checkpoint['epoch']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the model precision over some samples of the *remaining test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(remaining_testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "retrain_encoder.eval()\n",
    "retrain_classifier.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = retrain_encoder(images)\n",
    "    outputs = retrain_classifier(features)\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images[0:16]))\n",
    "\n",
    "# Prepare rows with GroundTruth and Predicted labels\n",
    "rows = [[\"GroundTruth\", *[classes[labels[j]] for j in range(16)]],\n",
    "        [\"Predicted\", *[classes[predicted[j]] for j in range(16)]]]\n",
    "\n",
    "# Display the table\n",
    "print(tabulate(rows, headers=[\"\"] + [f\"Image {i+1}\" for i in range(16)], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model overall performance on *remaining samples* and *unlearned samples*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_average_remaining_test_loss, retrain_best_remaining_test_acc = test(encoder=retrain_encoder, \n",
    "                                                                            classifier=retrain_classifier, \n",
    "                                                                            loader=remaining_testloader, \n",
    "                                                                            criterion=CRITERION, \n",
    "                                                                            hyperbolic='', \n",
    "                                                                            manifold=None, \n",
    "                                                                            device=device)\n",
    "\n",
    "retrain_average_unl_train_loss, retrain_best_unl_train_acc = test(encoder=retrain_encoder, \n",
    "                                                                classifier=retrain_classifier, \n",
    "                                                                loader=unlearning_trainloader, \n",
    "                                                                criterion=CRITERION, \n",
    "                                                                hyperbolic='', \n",
    "                                                                manifold=None, \n",
    "                                                                device=device)\n",
    "\n",
    "if SCENARIO == 'single-class':\n",
    "\n",
    "    retrain_average_unl_test_loss, retrain_best_unl_test_acc = test(encoder=retrain_encoder, \n",
    "                                                                    classifier=retrain_classifier, \n",
    "                                                                    loader=unlearning_testloader, \n",
    "                                                                    criterion=CRITERION, \n",
    "                                                                    hyperbolic='', \n",
    "                                                                    manifold=None, \n",
    "                                                                    device=device)\n",
    "    \n",
    "print(f'\\nRemaining Test Set Loss: {retrain_average_remaining_test_loss:.3f}, Accuracy: {retrain_best_remaining_test_acc:.3f}%')\n",
    "print(f'Unlearning Train Set Loss: {retrain_average_unl_train_loss:.3f}, Accuracy: {retrain_best_unl_train_acc:.3f}%')\n",
    "print(f'Unlearning Test Set Loss: {retrain_average_unl_test_loss:.3f}, Accuracy: {retrain_best_unl_test_acc:.3f}%') if SCENARIO == 'single-class' else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose which class samples to display by toggling the class names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCENARIO == 'single-class':\n",
    "\n",
    "    # Extract features from the test set\n",
    "    remaining_features, remaining_labels = extract_features(retrain_encoder, remaining_testloader, hyperbolic='', manifold=None, device=device)\n",
    "    unlearning_features, unlearning_labels = extract_features(retrain_encoder, unlearning_testloader, hyperbolic='', manifold=None, device=device)\n",
    "\n",
    "    # Combine for t-SNE\n",
    "    features = np.concatenate((unlearning_features, remaining_features), axis=0)\n",
    "    labels = np.concatenate((unlearning_labels, remaining_labels), axis=0)\n",
    "\n",
    "# In this case, there is no specific class to isolate, so the plotting goal is for all the models (original, retrain and unlearned) to plot a similar distribution of the whole test set\n",
    "elif SCENARIO == 'random-sample':\n",
    "\n",
    "    # Extract features from the test set\n",
    "    features, labels = extract_features(retrain_encoder, remaining_testloader, hyperbolic='', manifold=None, device=device)\n",
    "\n",
    "\n",
    "create_plot(features, labels, classes, dimension=2, convexhull=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlearning procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unlearning validation set configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unlearning algorithm takes as input:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The unlearning samples from the trainset\n",
    "2. The reamining samples from the trainset\n",
    "3. A validation set built like this:\n",
    "    - for *single-class* unlearning -> unlearning trainset samples\n",
    "    - for *random-sample* unlearning -> **subset**(unlearning trainset samples) + **subset**(original testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the contrastive unlearning procedure, the *unlearning validation set* is built following these rules, and it differs from the previous validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used only for the random-sample scenario\n",
    "def sample_validation_set(unlearning_trainset, testset, ratio=0.5):\n",
    "    \"\"\"\n",
    "    Create a two distinct validation sets for the sample unlearning case from a subset of \n",
    "    unlearning samples in the unlearning trainset and a subset of samples from the original test set.\n",
    "    \n",
    "    Args:\n",
    "    - unlearning_trainset: the dataset of unlearning samples (will take a Subset)\n",
    "    - testset: the original test set (will take a Subset)\n",
    "    - unlearning_trainset_indices: indices of the unlearning samples in the original trainset\n",
    "    - ratio: proportion of samples to use for the validation set (default is 10%)\n",
    "    \n",
    "    Returns:\n",
    "    - two eval_set: one for unlearning samples and one for test set samples\n",
    "    \"\"\"\n",
    "\n",
    "    # Randomly select a subset from the unlearning samples\n",
    "    total_indices = list(range(len(unlearning_trainset)))\n",
    "    eval_unlearning_trainset_indices = random.sample(total_indices, int(len(unlearning_trainset) * ratio))\n",
    "    \n",
    "    # Randomly select a subset from the test set\n",
    "    total_test_indices = list(range(len(testset)))\n",
    "    eval_test_indices = random.sample(total_test_indices, int(len(total_test_indices) * ratio))\n",
    "\n",
    "    return torch.utils.data.Subset(unlearning_trainset, eval_unlearning_trainset_indices), torch.utils.data.Subset(testset, eval_test_indices)\n",
    "\n",
    "if SCENARIO == 'single-class':\n",
    "    unl_valset_test = unlearning_testset\n",
    "    unl_valloader_test = unlearning_testloader\n",
    "\n",
    "elif SCENARIO == 'random-sample':\n",
    "    unl_subset_trainset, unl_subset_testset = sample_validation_set(unlearning_trainset, testset, ratio=0.5)\n",
    "    unl_valloader_train = torch.utils.data.DataLoader(unl_subset_trainset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    unl_valloader_test = torch.utils.data.DataLoader(unl_subset_testset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load original checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: the encoder and classifier here are the original ones (instantiated in the first part of the notebook).\n",
    "\n",
    "During the unlearning procedure, the original model parameters will be overwritten.\n",
    "\n",
    "So if you go back and try inference on the original model, the output will change (for worse, obviously).\n",
    "\n",
    "*This checkpoint cell is just for quick testing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Resuming from checkpoint...')\n",
    "\n",
    "assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "checkpoint = torch.load('./checkpoint/{}'.format(EUCL_ORIGINAL_CKPT_NAME))\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "classifier.load_state_dict(checkpoint['classifier'])\n",
    "\n",
    "print('Checkpoint loaded!')\n",
    "print(f'\\nValidation average loss: {checkpoint['loss']:.3f}')\n",
    "print(f'Validation set accuracy: {checkpoint['acc']:.3f}%')\n",
    "print(f'From epoch: {checkpoint['epoch']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive unlearning loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the custom ***Constrastive Unlearning*** loss as described in the original article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveUnlearningLoss(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super(ContrastiveUnlearningLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, embeddings_u, embeddings_r, labels_u, labels_r):\n",
    "        unlearning_batch_size = embeddings_u.size(0)\n",
    "        loss = 0.0\n",
    "\n",
    "        # Normalize embeddings\n",
    "        embeddings_u = F.normalize(embeddings_u, p=2, dim=1)\n",
    "        embeddings_r = F.normalize(embeddings_r, p=2, dim=1)\n",
    "\n",
    "        for i in range(unlearning_batch_size):\n",
    "            # Anchor: embedding of unlearning sample\n",
    "            z_u = embeddings_u[i]\n",
    "\n",
    "            if SCENARIO == 'single-class':\n",
    "                negatives = embeddings_r  # All remaining samples are negative\n",
    "                neg_sim = torch.exp(torch.matmul(z_u, negatives.T) / self.temperature)\n",
    "\n",
    "                denominator = neg_sim.size(0) + 1e-8\n",
    "\n",
    "                inner_loss = torch.log(neg_sim / denominator).sum() if len(neg_sim) > 0 else torch.tensor(0.0)\n",
    "                loss += (-1 / (neg_sim.size(0) + 1e-8)) * inner_loss\n",
    "\n",
    "                if len(neg_sim) == 0:\n",
    "                    print(\"Warning: neg_sim is zero or too small.\")\n",
    "\n",
    "            elif SCENARIO == 'random-sample':\n",
    "                pos_mask = labels_r == labels_u[i]\n",
    "                neg_mask = ~pos_mask\n",
    "\n",
    "                positives = embeddings_r[pos_mask]   # Same class as anchor\n",
    "                negatives = embeddings_r[neg_mask]   # Different class from anchor\n",
    "\n",
    "                # Calculate similarities\n",
    "                pos_sim = torch.exp(torch.matmul(z_u, positives.T) / self.temperature) if len(positives) > 0 else torch.tensor(0.0)\n",
    "                neg_sim = torch.exp(torch.matmul(z_u, negatives.T) / self.temperature)\n",
    "\n",
    "                # Updated denominator with both positive and negative similarities\n",
    "                denominator = pos_sim.sum() + 1e-8 \n",
    "\n",
    "                # Calculate inner loss\n",
    "                inner_loss = (torch.log(neg_sim / denominator)).sum() if len(neg_sim) > 0 else torch.tensor(0.0)\n",
    "\n",
    "                # Add to total loss\n",
    "                loss += (-1 / (neg_sim.size(0) + 1e-8)) * inner_loss  # Avoid divide-by-zero\n",
    "\n",
    "        return loss / unlearning_batch_size  # Average loss per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unlearning optimizer and scheduler configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here an assumption is made: the training hyperparamters are also good for unlearning.\n",
    "\n",
    "After some personal tests, it seems that this holds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST CONFIGURATION ADAM\n",
    "if OPTIMIZER_NAME == 'adam':\n",
    "    unl_optimizer = optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=0.000641388, weight_decay=0.567643)\t\n",
    "\n",
    "# BEST CONFIGURATION SGD\n",
    "elif OPTIMIZER_NAME == 'sgd':\n",
    "    unl_optimizer = optim.SGD(list(encoder.parameters()) + list(classifier.parameters()), lr=0.015254, momentum=0.582486, weight_decay=0.00366885) \n",
    "\n",
    "unl_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(unl_optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO NOT UNCOMMENT THESE: Unlearning hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Put large objects in Ray's object store for efficient memory management\n",
    "large_unlearning_trainloader = ray.put(unlearning_trainloader)\n",
    "if SCENARIO == 'random-sample':\n",
    "    large_unl_valloader_train = ray.put(unl_valloader_train)\n",
    "large_unl_valloader_test = ray.put(unl_valloader_test)\n",
    "large_remaining_trainset = ray.put(remaining_trainset)\n",
    "\n",
    "checkpoint = torch.load('./checkpoint/{}'.format(EUCL_ORIGINAL_CKPT_NAME))\n",
    "large_checkpoint = ray.put(checkpoint)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Calculate loss and accuracy of a data loader\n",
    "def get_loss_and_accuracy(encoder, classifier, data_loader, criterion):\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_loss = 0.0\n",
    "        encoder.eval()\n",
    "        classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in data_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = classifier(encoder(inputs))\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        accuracy = correct / total if total > 0 else 0\n",
    "        avg_loss = total_loss / len(data_loader) if len(data_loader) > 0 else 0\n",
    "        \n",
    "        return avg_loss, accuracy\n",
    "\n",
    "\n",
    "# Validation function to evaluate the unlearned efficency\n",
    "def validate_unlearning(encoder, classifier, valloader_train, valloader_test, classification_loss, class_count):\n",
    "\n",
    "    if SCENARIO == 'single-class':\n",
    "        # For single-class unlearning, accuracy on the unlearning class should be <= 1/class_count\n",
    "        unlearning_loss, unlearning_accuracy = get_loss_and_accuracy(encoder, classifier, valloader_test, classification_loss)\n",
    "        verdict = unlearning_accuracy <= 1 / class_count\n",
    "        #print('Unlearning class Loss: {:.4f}, Unlearning Accuracy: {:.4f}, Termination condition reached: {}'.format(unlearning_loss, unlearning_accuracy, verdict))\n",
    "    \n",
    "    elif SCENARIO == 'random-sample':\n",
    "        # For sample-unlearning, accuracy on unlearning samples <= accuracy on the test samples\n",
    "        unlearning_loss, unlearning_accuracy = get_loss_and_accuracy(encoder, classifier, valloader_train, classification_loss)\n",
    "        test_loss, test_accuracy = get_loss_and_accuracy(encoder, classifier, valloader_test, classification_loss)\n",
    "        verdict = unlearning_accuracy <= test_accuracy\n",
    "        #print('Unlearning sample Loss: {:.4f}, Unlearning Accuracy: {:.4f}, Test loss: {:.4f}, Test accuracy: {:.4f}, Termination condition reached: {}'.format(unlearning_loss, unlearning_accuracy, test_loss, test_accuracy, verdict))\n",
    "\n",
    "    return unlearning_accuracy, verdict\n",
    "    \n",
    "\n",
    "\n",
    "# Validation function to evaluate the remaining accuracy\n",
    "def validate_containment(encoder, classifier, remaining_testloader, classification_loss, class_count):\n",
    "\n",
    "    # For single-class unlearning, accuracy on the unlearning class should be <= 1/class_count\n",
    "    remaining_loss, remaining_accuracy = get_loss_and_accuracy(encoder, classifier, remaining_testloader, classification_loss)\n",
    "    #print('Remaining Test Set Loss: {:.4f}, Accuracy: {:.4f}'.format(remaining_loss, remaining_accuracy))\n",
    "    return remaining_accuracy\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Main training loop with validation\n",
    "def train_contrastive_unlearning(config):\n",
    "\n",
    "    # DA TOGLIERE INSIEME A DATA PARALLEL\n",
    "    def remove_module_prefix(state_dict):\n",
    "        return {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "    \n",
    "    scaler = torch.amp.grad_scaler.GradScaler()\n",
    "\n",
    "    encoder, classifier = ResNet34()\n",
    "\n",
    "    checkpoint = ray.get(large_checkpoint)\n",
    "\n",
    "    encoder_state = remove_module_prefix(checkpoint['encoder'])\n",
    "    classifier_state = remove_module_prefix(checkpoint['classifier'])\n",
    "\n",
    "    encoder.load_state_dict(encoder_state)\n",
    "    classifier.load_state_dict(classifier_state)\n",
    "\n",
    "    encoder = encoder.to(device)\n",
    "    classifier = classifier.to(device)\n",
    "\n",
    "    unlearning_trainloader = ray.get(large_unlearning_trainloader)\n",
    "    remaining_trainset = ray.get(large_remaining_trainset)\n",
    "    if SCENARIO == 'random-sample':\n",
    "        unl_valloader_train = ray.get(large_unl_valloader_train)\n",
    "    unl_valloader_test = ray.get(large_unl_valloader_test)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    if config['opt'] == 'sgd':\n",
    "        optimizer = optim.SGD(list(encoder.parameters()) + list(classifier.parameters()), lr=0.015254, momentum=0.582486, weight_decay=0.00366885)\n",
    "    elif config['opt'] == 'adam':\n",
    "        optimizer = optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=0.000641388, weight_decay=0.567643)\n",
    "    \n",
    "    class_count = 10\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "\n",
    "        encoder.train()\n",
    "        classifier.train()\n",
    "\n",
    "        total_loss_epoch = 0.0  # To accumulate loss over the epoch\n",
    "        total_batches = 0\n",
    "        \n",
    "        # Iterate over unlearning dataset (D_u)\n",
    "        for x_u, y_u in unlearning_trainloader:\n",
    "            x_u, y_u = x_u.to(device), y_u.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = 0.0  # To accumulate loss over the batch\n",
    "            \n",
    "            # Perform  iterations over the remaining dataset for each unlearning batch\n",
    "            for _ in range(config[\"omega\"]):  # Omega loop\n",
    "\n",
    "                # Create a DataLoader with a RandomSampler\n",
    "                random_sampler = torch.utils.data.RandomSampler(remaining_trainset)\n",
    "                remaining_trainset_loader = torch.utils.data.DataLoader(remaining_trainset, sampler=random_sampler, batch_size=128, pin_memory=True)\n",
    "\n",
    "                for remaining_data in remaining_trainset_loader:\n",
    "                    x_r, y_r = remaining_data\n",
    "                    break\n",
    "\n",
    "                x_r, y_r = x_r.to(device), y_r.to(device)\n",
    "\n",
    "                # Use mixed precision\n",
    "                with torch.amp.autocast_mode.autocast(device_type=device, dtype=torch.float16):\n",
    "                    # Forward pass for remaining samples (classification loss)\n",
    "                    z_r = encoder(x_r) \n",
    "                    logits_r = classifier(z_r)\n",
    "\n",
    "                    # Forward pass for unlearning samples\n",
    "                    z_u = encoder(x_u)\n",
    "                    logits_u = classifier(z_u)\n",
    "\n",
    "                ce_loss = criterion(logits_r, y_r)\n",
    "                # Compute contrastive unlearning loss\n",
    "                loss_fn = ContrastiveUnlearningLoss(temperature=config[\"temperature\"])\n",
    "                ul_loss = loss_fn(z_u, z_r, y_u, y_r)\n",
    "\n",
    "                # Total loss: classification loss + contrastive unlearning loss\n",
    "                total_loss = config[\"regularizer_ul\"] * ul_loss + config[\"regularizer_ce\"] * ce_loss\n",
    "                batch_loss += total_loss.item()\n",
    "\n",
    "                scaler.scale(total_loss).backward()\n",
    "                \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_batches += 1\n",
    "            total_loss_epoch += batch_loss    \n",
    "            \n",
    "        # Compute validation for termination criteria\n",
    "        if SCENARIO == 'single-class':\n",
    "            unlearned_acc, verdict = validate_unlearning(encoder, classifier, None, unl_valloader_test, CRITERION, class_count=class_count)\n",
    "        elif SCENARIO == 'random-sample':\n",
    "            unlearned_acc, verdict = validate_unlearning(encoder, classifier, unl_valloader_train, unl_valloader_test, CRITERION, class_count=class_count)\n",
    "\n",
    "        # Compute accuracy retain over the remaining samples\n",
    "        remaining_acc = validate_containment(encoder, classifier, remaining_testloader, CRITERION, class_count=class_count)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # report results to Ray Tune\n",
    "        ray.train.report(dict(unlearned_accuracy=unlearned_acc, remaining_accuracy=remaining_acc))       \n",
    "\n",
    "        # Termination criteria\n",
    "        if verdict: \n",
    "            print(\"\\nTerminating unlearning procedure\")\n",
    "            break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def custom_trial_name(trial):\n",
    "    return f\"trial_unlearning{trial.trial_id}\"\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    \"num_epochs\": tune.grid_search([NUM_EPOCHS]),\n",
    "    \"opt\": tune.grid_search(['sgd']),\n",
    "    \"temperature\": tune.uniform(0.1, 1.0),\n",
    "    \"regularizer_ce\": tune.uniform(0.1, 1.0),\n",
    "    \"regularizer_ul\": tune.uniform(0.1, 1.0),\n",
    "    \"omega\": tune.choice([2, 4, 6]),\n",
    "}\n",
    "\n",
    "# Use partial to pass fixed_params\n",
    "analysis = tune.run(\n",
    "    train_contrastive_unlearning,\n",
    "    resources_per_trial={\"cpu\": 20, \"gpu\": 1},\n",
    "    config=search_space,\n",
    "    num_samples=50,\n",
    "    scheduler=ASHAScheduler(metric=\"remaining_accuracy\", mode=\"max\"),    \n",
    "    trial_dirname_creator=custom_trial_name\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Unlearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two validation functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *validate_unlearning* test how efficient the unlearning is\n",
    "- *validate_containment* test how efficiently the model mantain knowledge on the rest of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss and accuracy\n",
    "def get_loss_and_accuracy(encoder, classifier, data_loader, criterion, set_name='', hyperbolic='', manifold=None, device='cuda'):\n",
    "    \"\"\"\n",
    "    Validate the unlearning process based on the scenario.\n",
    "    Args:\n",
    "        encoder: The encoder model to extract features.\n",
    "        classifier: The classifier head model for predictions.\n",
    "        data_loader: The loader of a specific dataset. \n",
    "        criterion: Loss function (e.g., CrossEntropyLoss).\n",
    "        set_name: the name of the set it is running (for the tqdm bar)\n",
    "    \n",
    "    Returns:\n",
    "        avg_loss: The average loss of the model on the data_loader\n",
    "        accuracy: The accuracy of the model on the data_loader\n",
    "    \"\"\"\n",
    "\n",
    "    progress_bar_val = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in progress_bar_val:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # The projection is needed ony if the whole model is hyperbolic\n",
    "            if hyperbolic in ['complete-ResNet', 'academic-ResNet']:\n",
    "                # move the inputs to the manifold\n",
    "                tangents = TangentTensor(data=inputs, man_dim=1, manifold=manifold)\n",
    "                inputs = manifold.expmap(tangents)\n",
    "\n",
    "            outputs = classifier(encoder(inputs))\n",
    "\n",
    "            # Indipendently of the hyperbolic model, we need to extract the tensor  \n",
    "            if hyperbolic in ['complete-ResNet', 'hybrid-ResNet', 'academic-ResNet']:\n",
    "                # Needed for the loss computation\n",
    "                outputs = outputs.tensor # This gives the underlying PyTorch tensor\n",
    "                \n",
    "            loss = criterion(outputs, labels)\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            progress_bar_val.set_description('UNLEARNING VAL | %s | Loss: %.3f, Accuracy: %.3f%% (%d/%d)' % (set_name, total_loss/(batch_idx+1), 100. * correct/total, correct, total))\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    avg_loss = total_loss / len(data_loader) if len(data_loader) > 0 else 0\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "\n",
    "# Validation function to evaluate the unlearned efficency\n",
    "def validate_unlearning(encoder, classifier, valloader_train, valloader_test, classification_loss, class_count, hyperbolic='', manifold=None, device='cuda'):\n",
    "    \"\"\"\n",
    "    Validate the unlearning process based on the scenario.\n",
    "    Args:\n",
    "        encoder: The encoder model to extract features.\n",
    "        classifier: The classifier head model for predictions.\n",
    "        valloader_train: the unlearning trainset subset DataLoader.\n",
    "        valloader_test: the testset subset DataLoader.\n",
    "        classification_loss: Loss function (e.g., CrossEntropyLoss).\n",
    "        class_count: Total number of classes in the dataset (for random guess accuracy threshold).\n",
    "    \n",
    "    Returns:\n",
    "        unlearning_accuracy: the accuracy of the model on the unlearning validation set\n",
    "        verdict: A boolean value indicating if the termination condition is met.\n",
    "    \"\"\"\n",
    "\n",
    "    if SCENARIO == 'single-class':\n",
    "        # For single-class unlearning, accuracy on the unlearning class should be <= 1/class_count\n",
    "        unlearning_loss, unlearning_accuracy = get_loss_and_accuracy(encoder, classifier, valloader_test, classification_loss, set_name='Unlearning Test Set', hyperbolic=hyperbolic, manifold=manifold, device=device)\n",
    "        verdict = unlearning_accuracy <= 1 / class_count\n",
    "        \n",
    "    \n",
    "    elif SCENARIO == 'random-sample':\n",
    "        # For sample-unlearning, accuracy on unlearning samples <= accuracy on the test samples\n",
    "        unlearning_loss, unlearning_accuracy = get_loss_and_accuracy(encoder, classifier, valloader_train, classification_loss, set_name='Unlearning Train Subset', hyperbolic=hyperbolic, manifold=manifold, device=device)\n",
    "        test_loss, test_accuracy = get_loss_and_accuracy(encoder, classifier, valloader_test, classification_loss, set_name='Test Subset', hyperbolic=hyperbolic, manifold=manifold, device=device)\n",
    "        verdict = unlearning_accuracy <= test_accuracy\n",
    "        \n",
    "    return unlearning_accuracy, verdict\n",
    "    \n",
    "\n",
    "\n",
    "# Validation function to evaluate the remaining accuracy\n",
    "def validate_containment(encoder, classifier, remaining_testloader, classification_loss, class_count, hyperbolic='', manifold=None, device='cuda'):\n",
    "    \"\"\"\n",
    "    Validate the accuracy on the remaining samples based on the scenario.\n",
    "    Args:\n",
    "        encoder: The encoder model to extract features.\n",
    "        classifier: The classifier head model for predictions.\n",
    "        remaining_testloader: the loader of the remaining samples of the test set\n",
    "        classification_loss: Loss function (e.g., CrossEntropyLoss).\n",
    "        class_count: Total number of classes in the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        A boolean value indicating if the termination condition is met.\n",
    "    \"\"\"\n",
    "\n",
    "    # For single-class unlearning, accuracy on the unlearning class should be <= 1/class_count\n",
    "    remaining_loss, remaining_accuracy = get_loss_and_accuracy(encoder, classifier, remaining_testloader, classification_loss, set_name='Remaining Test Set', hyperbolic=hyperbolic, manifold=manifold, device=device)\n",
    "    \n",
    "    return remaining_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unlearning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop with validation\n",
    "def train_contrastive_unlearning(encoder, classifier, unlearning_trainloader, remaining_trainset, classification_loss, unlearning_loss, \n",
    "                                 optimizer, omega, lambda_ce, lambda_ul, scaler, hyperbolic='', manifold=None, device='cuda'):\n",
    "\n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "\n",
    "    progress_bar_unlearn = tqdm(enumerate(unlearning_trainloader), total=len(unlearning_trainloader))\n",
    "\n",
    "    total_loss_epoch = 0.0  # To accumulate loss over the epoch\n",
    "    total_batches = 0\n",
    "    \n",
    "    # Iterate over unlearning dataset (D_u)\n",
    "    for batch_idx, (x_u, y_u) in progress_bar_unlearn:\n",
    "        x_u, y_u = x_u.to(device), y_u.to(device)\n",
    "\n",
    "        # The projection is needed ony if the whole model is hyperbolic\n",
    "        if hyperbolic in ['complete-ResNet', 'academic-ResNet']:\n",
    "            # move the inputs to the manifold\n",
    "            tangents = TangentTensor(data=x_u, man_dim=1, manifold=manifold)\n",
    "            x_u = manifold.expmap(tangents)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = 0.0  # To accumulate loss over the batch\n",
    "        \n",
    "        # Perform  iterations over the remaining dataset for each unlearning batch\n",
    "        for _ in range(omega):  # Omega loop\n",
    "\n",
    "            # Create a DataLoader with a RandomSampler\n",
    "            random_sampler = torch.utils.data.RandomSampler(remaining_trainset)\n",
    "            remaining_trainset_loader = torch.utils.data.DataLoader(remaining_trainset, sampler=random_sampler, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "            for remaining_data in remaining_trainset_loader:\n",
    "                x_r, y_r = remaining_data\n",
    "                break\n",
    "\n",
    "            x_r, y_r = x_r.to(device), y_r.to(device)\n",
    "\n",
    "            # The projection is needed ony if the whole model is hyperbolic\n",
    "            if hyperbolic in ['complete-ResNet', 'academic-ResNet']:\n",
    "                # move the inputs to the manifold\n",
    "                tangents = TangentTensor(data=x_r, man_dim=1, manifold=manifold)\n",
    "                x_r = manifold.expmap(tangents)\n",
    "\n",
    "            # Use mixed precision\n",
    "            with torch.amp.autocast_mode.autocast(device_type=device, dtype=torch.float16):\n",
    "                # Forward pass for remaining samples (classification loss)\n",
    "                z_r = encoder(x_r) \n",
    "                logits_r = classifier(z_r)\n",
    "\n",
    "                # Forward pass for unlearning samples\n",
    "                z_u = encoder(x_u)\n",
    "                logits_u = classifier(z_u)\n",
    "\n",
    "            # Compute classification loss\n",
    "            # Indipendently of the hyperbolic model, we need to extract the tensor  \n",
    "            if hyperbolic in ['complete-ResNet', 'hybrid-ResNet', 'academic-ResNet']:\n",
    "                # Needed for the loss computation\n",
    "                logits_r = logits_r.tensor # This gives the underlying PyTorch tensor\n",
    "                \n",
    "            ce_loss = classification_loss(logits_r, y_r)\n",
    "            \n",
    "            # Compute contrastive unlearning loss\n",
    "            #loss_fn = ContrastiveUnlearningLoss(temperature=temperature)\n",
    "            ul_loss = unlearning_loss(z_u, z_r, y_u, y_r)\n",
    "\n",
    "            # Total loss: classification loss + contrastive unlearning loss\n",
    "            total_loss = lambda_ul * ul_loss + lambda_ce * ce_loss\n",
    "            batch_loss += total_loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            scaler.scale(total_loss).backward()\n",
    "            \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_batches += 1\n",
    "        total_loss_epoch += batch_loss\n",
    "            \n",
    "        # Update progress bar description\n",
    "        progress_bar_unlearn.set_description('UNLEARNING TRAIN | Batch Loss: %.4f | Average Loss: %.4f' % (batch_loss, total_loss_epoch / len(unlearning_trainloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.amp.grad_scaler.GradScaler()\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "\n",
    "    train_contrastive_unlearning(encoder=encoder, \n",
    "                                 classifier=classifier, \n",
    "                                 unlearning_trainloader=unlearning_trainloader, \n",
    "                                 remaining_trainset=remaining_trainset, \n",
    "                                 classification_loss=CRITERION, \n",
    "                                 unlearning_loss=ContrastiveUnlearningLoss(temperature=TEMPERATURE),\n",
    "                                 optimizer=unl_optimizer, \n",
    "                                 omega=OMEGA, \n",
    "                                 lambda_ce=REGULARIZER_CE, \n",
    "                                 lambda_ul=REGULARIZER_UL,  \n",
    "                                 scaler=scaler,\n",
    "                                 hyperbolic='',\n",
    "                                 manifold=None,\n",
    "                                 device=device\n",
    "                                 )\n",
    "\n",
    "    if SCENARIO == 'single-class':\n",
    "        unlearned_acc, verdict = validate_unlearning(encoder=encoder, \n",
    "                                                     classifier=classifier, \n",
    "                                                     valloader_train=None, \n",
    "                                                     valloader_test=unl_valloader_test, \n",
    "                                                     classification_loss=CRITERION, \n",
    "                                                     class_count=len(classes),\n",
    "                                                     hyperbolic='',\n",
    "                                                     manifold=None,\n",
    "                                                     device=device\n",
    "                                                     )\n",
    "        \n",
    "        remaining_acc = validate_containment(encoder=encoder, \n",
    "                                             classifier=classifier, \n",
    "                                             remaining_testloader=remaining_testloader, \n",
    "                                             classification_loss=CRITERION, \n",
    "                                             class_count=len(classes), \n",
    "                                             device=device\n",
    "                                             )\n",
    "\n",
    "    elif SCENARIO == 'random-sample':\n",
    "        unlearned_acc, verdict = validate_unlearning(encoder=encoder, \n",
    "                                                     classifier=classifier, \n",
    "                                                     valloader_train=unl_valloader_train, \n",
    "                                                     valloader_test=unl_valloader_test, \n",
    "                                                     classification_loss=CRITERION, \n",
    "                                                     class_count=len(classes),\n",
    "                                                     hyperbolic='',\n",
    "                                                     manifold=None,\n",
    "                                                     device=device\n",
    "                                                     )\n",
    "    \n",
    "    print(f\"\\nTermination condition reached: {verdict}\")\n",
    "\n",
    "    # Termination criteria\n",
    "    if verdict: \n",
    "        print(\"\\nTerminating unlearning procedure\")\n",
    "        break\n",
    "\n",
    "    unl_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of unlearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unlearning efficiency is tested on the same subsets used by the authors for the experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *single-class* scenario --> remaining test set, unlearning train set, unleraning test set\n",
    "- *random-sample* scenario --> remaining test set, unlearning train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlearned_average_remaining_test_loss, unlearned_best_remaining_test_acc = test(encoder=encoder, \n",
    "                                                                                classifier=classifier, \n",
    "                                                                                loader=remaining_testloader, \n",
    "                                                                                criterion=CRITERION, \n",
    "                                                                                hyperbolic='', \n",
    "                                                                                manifold=None, \n",
    "                                                                                device=device)\n",
    "\n",
    "unlearned_average_unl_train_loss, unlearned_best_unl_train_acc = test(encoder=encoder, \n",
    "                                                                        classifier=classifier, \n",
    "                                                                        loader=unlearning_trainloader, \n",
    "                                                                        criterion=CRITERION, \n",
    "                                                                        hyperbolic='', \n",
    "                                                                        manifold=None, \n",
    "                                                                        device=device)\n",
    "\n",
    "if SCENARIO == 'single-class':\n",
    "    \n",
    "    unlearned_average_unl_test_loss, unlearned_best_unl_test_acc = test(encoder=encoder, \n",
    "                                                                        classifier=classifier, \n",
    "                                                                        loader=unlearning_testloader, \n",
    "                                                                        criterion=CRITERION, \n",
    "                                                                        hyperbolic='', \n",
    "                                                                        manifold=None, \n",
    "                                                                        device=device)\n",
    "\n",
    "print(f'\\nUnlearned Remaining Test Set Loss: {unlearned_average_remaining_test_loss:.3f}, Accuracy: {unlearned_best_remaining_test_acc:.3f}%')\n",
    "print(f'Unlearned Unlearning Train Set Loss: {unlearned_average_unl_train_loss:.3f}, Accuracy: {unlearned_best_unl_train_acc:.3f}%')\n",
    "print(f'Unlearned Unlearning Test Set Loss: {unlearned_average_unl_test_loss:.3f}, Accuracy: {unlearned_best_unl_test_acc:.3f}%') if SCENARIO == 'single-class' else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions over batch sample of the *remaining test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(remaining_testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "encoder.eval()\n",
    "classifier.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = encoder(images)\n",
    "    outputs = classifier(features)\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images[0:16]))\n",
    "\n",
    "# Prepare rows with GroundTruth and Predicted labels\n",
    "rows = [[\"GroundTruth\", *[classes[labels[j]] for j in range(16)]],\n",
    "        [\"Predicted\", *[classes[predicted[j]] for j in range(16)]]]\n",
    "\n",
    "# Display the table\n",
    "print(tabulate(rows, headers=[\"\"] + [f\"Image {i+1}\" for i in range(16)], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions over batch sample of the *unlearned set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCENARIO == 'single-class':\n",
    "\n",
    "    dataiter = iter(unlearning_testloader)\n",
    "    images, labels = next(dataiter)\n",
    "\n",
    "elif SCENARIO == 'random-sample':\n",
    "\n",
    "    dataiter = iter(unlearning_trainloader)\n",
    "    images, labels = next(dataiter)\n",
    "\n",
    "encoder.eval()\n",
    "classifier.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = encoder(images)\n",
    "    outputs = classifier(features)\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images[0:16]))\n",
    "\n",
    "# Prepare rows with GroundTruth and Predicted labels\n",
    "rows = [[\"GroundTruth\", *[classes[labels[j]] for j in range(16)]],\n",
    "        [\"Predicted\", *[classes[predicted[j]] for j in range(16)]]]\n",
    "\n",
    "# Display the table\n",
    "print(tabulate(rows, headers=[\"\"] + [f\"Image {i+1}\" for i in range(16)], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCENARIO == 'single-class':\n",
    "\n",
    "    # Extract features from the test set\n",
    "    remaining_features, remaining_labels = extract_features(encoder, remaining_testloader, hyperbolic='', manifold=None, device=device)\n",
    "    unlearned_features, unlearned_labels = extract_features(encoder, unlearning_testloader, hyperbolic='', manifold=None, device=device)\n",
    "\n",
    "    # Combine for t-SNE\n",
    "    features = np.concatenate((unlearned_features, remaining_features), axis=0)\n",
    "    labels = np.concatenate((unlearned_labels, remaining_labels), axis=0)\n",
    "\n",
    "# In this case, there is no specific class to isolate, so the plotting goal is for all the models (original, retrain and unlearned) to plot a similar distribution of the whole test set\n",
    "elif SCENARIO == 'random-sample':\n",
    "\n",
    "    features, labels = extract_features(encoder, remaining_testloader, hyperbolic='', manifold=None, device=device)\n",
    "    \n",
    "\n",
    "create_plot(features, labels, classes, dimension=2, convexhull=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Contrastive Unlearning Pipeline on Hyperbolic Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this macro-section i will repeat the whole setup buth using **hyperbolic spaces**.\n",
    "\n",
    "The library used for experimenting with hyperbolic spaces is [***Hypll***](https://github.com/maxvanspengler/hyperbolic_learning_library)\n",
    "\n",
    "The first one is mainly used for building the hyperbolic architecture, while the second one for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: to run this second chapter, you NEED to have the *Chapter 1* executed, because the data and most of the functions used here are defined in previous cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperbolic original model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the most logical thing to do is to take the original architecture and make it hyperbolic.\n",
    "\n",
    "However hyperbolic operations are exceptionaly **slow** and **heavy**, so i needed to figure out something else...\n",
    "\n",
    "I ended up with 3 different architectures. You will find explenation for each one of them down below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete hyperbolic version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the original architecture (defined in the *Original model* section of chapter 1) made completely hyperbolic.\n",
    "\n",
    "This means that every single operation is defined to work in hyperbolic spaces.\n",
    "\n",
    "This is the slowest and heaviest of the 3 architecture. \n",
    "\n",
    "On my computer i had to decrease the batch size from 128 to 32 and it takes around 1 hour to make a single epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperbolicBasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, manifold, stride=1):\n",
    "        super(HyperbolicBasicBlock, self).__init__()\n",
    "        self.manifold = manifold\n",
    "        \n",
    "        self.conv1 = hnn.HConvolution2d(in_planes, planes, kernel_size=3,stride=stride, \n",
    "                                        padding=1, bias=False, manifold=manifold)\n",
    "        self.bn1 = hnn.HBatchNorm2d(planes, manifold=manifold)\n",
    "        self.relu = hnn.HReLU(manifold=manifold)\n",
    "        self.conv2 = hnn.HConvolution2d(planes, planes, kernel_size=3, stride=1, \n",
    "                                        padding=1, bias=False, manifold=manifold)\n",
    "        self.bn2 = hnn.HBatchNorm2d(planes, manifold=manifold)\n",
    "        #self.dropout = nn.Dropout(p=0.3)  # Add dropout layer\n",
    "        \n",
    "        # Shortcut layer for downsampling when needed\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                hnn.HConvolution2d(in_planes, self.expansion * planes, kernel_size=1, \n",
    "                                   stride=stride, bias=False, manifold=manifold),\n",
    "                hnn.HBatchNorm2d(features=self.expansion * planes, manifold=manifold)\n",
    "            )\n",
    "\n",
    "    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        #out = self.dropout(out)  # Apply dropout after the last batch norm\n",
    "        \n",
    "        # Apply Mbius addition for the residual connection\n",
    "        out = self.manifold.mobius_add(out, self.shortcut(x))\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "class HyperbolicBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, manifold, in_planes, planes, stride=1):\n",
    "        super(HyperbolicBottleneck, self).__init__()\n",
    "        self.manifold = manifold\n",
    "        self.conv1 = hnn.HConvolution2d(in_planes, planes, kernel_size=1, \n",
    "                                        bias=False, manifold=manifold)\n",
    "        self.bn1 = hnn.HBatchNorm2d(planes, manifold)\n",
    "        self.relu = hnn.HReLU(manifold=manifold)\n",
    "        self.conv2 = hnn.HConvolution2d(planes, planes, kernel_size=3, stride=stride, \n",
    "                                        padding=1, bias=False, manifold=manifold)\n",
    "        self.bn2 = hnn.HBatchNorm2d(planes, manifold)\n",
    "        self.conv3 = hnn.HConvolution2d(planes, self.expansion * planes, kernel_size=1, \n",
    "                                        bias=False, manifold=manifold)\n",
    "        self.bn3 = hnn.HBatchNorm2d(self.expansion * planes, manifold=manifold)\n",
    "        #self.dropout = nn.Dropout(p=0.3)  # Add dropout layer\n",
    "\n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                hnn.HConvolution2d(in_planes, self.expansion * planes, kernel_size=1, \n",
    "                                   stride=stride, bias=False, manifold=manifold),\n",
    "                hnn.HBatchNorm2d(self.expansion * planes, manifold=manifold)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        #out = self.dropout(out)  # Apply dropout after the last batch norm\n",
    "\n",
    "        # Apply Mbius addition for the residual connection\n",
    "        out = self.manifold.mobius_add(out, self.shortcut(x))\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class HyperbolicResNetEncoder(nn.Module):\n",
    "    def __init__(self, block, num_blocks, manifold, num_classes=10):\n",
    "        super(HyperbolicResNetEncoder, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.manifold = manifold\n",
    "\n",
    "        self.conv1 = hnn.HConvolution2d(3, 64, kernel_size=3, stride=1, \n",
    "                                        padding=1, bias=False, manifold=manifold)\n",
    "        self.bn1 = hnn.HBatchNorm2d(64, manifold=manifold)\n",
    "        self.relu = hnn.HReLU(manifold=manifold)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.dropout = nn.Dropout(p=0.3)  # Add Dropout layer\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, self.manifold, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = hnn.HAvgPool2d(kernel_size=4, manifold=self.manifold)(out)\n",
    "        out = hnn.HFlatten()(out)\n",
    "        #out = self.dropout(out) # Add Dropout layer\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class HyperbolicClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim, manifold, num_classes=10):\n",
    "        super(HyperbolicClassificationHead, self).__init__()\n",
    "        #self.dropout = nn.Dropout(p=0.5) # Add Dropout layer\n",
    "        self.fc = hnn.HLinear(input_dim, num_classes, manifold=manifold)\n",
    "\n",
    "\n",
    "    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n",
    "        #x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "\n",
    "def HyperbolicResNet18(manifold):\n",
    "    encoder = HyperbolicResNetEncoder(HyperbolicBasicBlock, [2, 2, 2, 2], manifold)\n",
    "    classification = HyperbolicClassificationHead(512, manifold)\n",
    "    return encoder, classification\n",
    "\n",
    "\n",
    "def HyperbolicResNet34(manifold):\n",
    "    encoder = HyperbolicResNetEncoder(HyperbolicBasicBlock, [3, 4, 6, 3], manifold)\n",
    "    classification = HyperbolicClassificationHead(512, manifold)\n",
    "    return encoder, classification\n",
    "\n",
    "\n",
    "def HyperbolicResNet50(manifold):\n",
    "    encoder = HyperbolicResNetEncoder(HyperbolicBottleneck, [3, 4, 6, 3], manifold)\n",
    "    classification = HyperbolicClassificationHead(2048, manifold)\n",
    "    return encoder, classification\n",
    "\n",
    "\n",
    "def HyperbolicResNet101(manifold):\n",
    "    encoder = HyperbolicResNetEncoder(HyperbolicBottleneck, [3, 4, 23, 3], manifold)\n",
    "    classification = HyperbolicClassificationHead(2048, manifold)\n",
    "    return encoder, classification\n",
    "\n",
    "\n",
    "def HyperbolicResNet152(manifold):\n",
    "    encoder = HyperbolicResNetEncoder(HyperbolicBottleneck, [3, 8, 36, 3], manifold)\n",
    "    classification = HyperbolicClassificationHead(2048, manifold)\n",
    "    return encoder, classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid hyperbolic (final embedding layer + classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the original architecture (defined in the *Original model* section of chapter 1) with hyperbolic mapping for the final embeddings.\n",
    "\n",
    "This means the features are computed using standard Euclidean operations and only at the end they are projected into an hyperbolic space (*Poincar ball*).\n",
    "\n",
    "Also the classification head is hyperbolic.\n",
    "\n",
    "This is the fastest of the 3 architectures, and the one that i used for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.dropout = nn.Dropout(p=0.3)  # Add dropout layer\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.dropout(out)  # Apply dropout after the last batch norm\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
    "        self.dropout = nn.Dropout(p=0.3)  # Add dropout layer\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out = self.dropout(out)  # Apply dropout after the last batch norm\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, block, num_blocks, manifold, num_classes=10):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.manifold = manifold\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.dropout = nn.Dropout(p=0.3)  # Add Dropout layer\n",
    "        \n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dropout(out) # Add Dropout layer\n",
    "\n",
    "        # Projects the embeddings in the Poincar manifold\n",
    "        out = TangentTensor(data=out, man_dim=1, manifold=self.manifold)\n",
    "        out = self.manifold.expmap(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class HyperbolicClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim, manifold, num_classes=10):\n",
    "        super(HyperbolicClassificationHead, self).__init__()\n",
    "        self.manifold = manifold\n",
    "        self.dropout = nn.Dropout(p=0.5) # Add Dropout layer\n",
    "        self.fc = hnn.HLinear(input_dim, num_classes, manifold=manifold)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x.tensor)\n",
    "        # Projects the embeddings back in the Poincar manifold\n",
    "        x = TangentTensor(data=x, man_dim=1, manifold=self.manifold)\n",
    "        x = self.manifold.expmap(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def HybridHyperbolicResNet18(manifold):\n",
    "    encoder = ResNetEncoder(BasicBlock, [2, 2, 2, 2], manifold)\n",
    "    classification = HyperbolicClassificationHead(512, manifold, num_classes=len(classes))\n",
    "    return encoder, classification\n",
    "\n",
    "\n",
    "def HybridHyperbolicResNet34(manifold):\n",
    "    encoder = ResNetEncoder(BasicBlock, [3, 4, 6, 3], manifold)\n",
    "    classification = HyperbolicClassificationHead(512, manifold, num_classes=len(classes))\n",
    "    return encoder, classification\n",
    "\n",
    "\n",
    "def HybridHyperbolicResNet50(manifold):\n",
    "    encoder = ResNetEncoder(Bottleneck, [3, 4, 6, 3], manifold)\n",
    "    classification = HyperbolicClassificationHead(2048, manifold, num_classes=len(classes))\n",
    "    return encoder, classification\n",
    "\n",
    "\n",
    "def HybridHyperbolicResNet101(manifold):\n",
    "    encoder = ResNetEncoder(Bottleneck, [3, 4, 23, 3], manifold)\n",
    "    classification = HyperbolicClassificationHead(2048, manifold, num_classes=len(classes))\n",
    "    return encoder, classification\n",
    "\n",
    "\n",
    "def HybridHyperbolicResNet152(manifold):\n",
    "    encoder = ResNetEncoder(Bottleneck, [3, 8, 36, 3], manifold)\n",
    "    classification = HyperbolicClassificationHead(2048, manifold, num_classes=len(classes))\n",
    "    return encoder, classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggested hypll model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Hypll library implementation of a hyperbolic classifier.\n",
    "\n",
    "However, this is not a random classifier. \n",
    "\n",
    "It is based on the [*Poincar ResNet paper*](https://arxiv.org/abs/2303.14027) which, in turn, is based on the original Euclidean implementation described in the paper [*Deep Residual Learning for Image Recognition*](https://arxiv.org/abs/1512.03385).\n",
    "\n",
    "It was already built for the CIFAR-10 dataset, so i simply took it and devided it into encoder and classifier.\n",
    "\n",
    "This model has a decent speed, but still nowhere near close to the Hybrid version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoincareResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        manifold: PoincareBall,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Sequential] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.manifold = manifold\n",
    "        self.stride = stride\n",
    "        self.downsample = downsample\n",
    "\n",
    "        self.conv1 = hnn.HConvolution2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            manifold=manifold,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.bn1 = hnn.HBatchNorm2d(features=out_channels, manifold=manifold)\n",
    "        self.relu = hnn.HReLU(manifold=self.manifold)\n",
    "        self.conv2 = hnn.HConvolution2d(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            manifold=manifold,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.bn2 = hnn.HBatchNorm2d(features=out_channels, manifold=manifold)\n",
    "\n",
    "    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(residual)\n",
    "\n",
    "        x = self.manifold.mobius_add(x, residual)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class PoincareEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        channel_sizes: list[int],\n",
    "        group_depths: list[int],\n",
    "        manifold: PoincareBall,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channel_sizes = channel_sizes\n",
    "        self.group_depths = group_depths\n",
    "        self.manifold = manifold\n",
    "\n",
    "        self.conv = hnn.HConvolution2d(\n",
    "            in_channels=3,\n",
    "            out_channels=channel_sizes[0],\n",
    "            kernel_size=3,\n",
    "            manifold=manifold,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.bn = hnn.HBatchNorm2d(features=channel_sizes[0], manifold=manifold)\n",
    "        self.relu = hnn.HReLU(manifold=manifold)\n",
    "        self.group1 = self._make_group(\n",
    "            in_channels=channel_sizes[0],\n",
    "            out_channels=channel_sizes[0],\n",
    "            depth=group_depths[0],\n",
    "        )\n",
    "        self.group2 = self._make_group(\n",
    "            in_channels=channel_sizes[0],\n",
    "            out_channels=channel_sizes[1],\n",
    "            depth=group_depths[1],\n",
    "            stride=2,\n",
    "        )\n",
    "        self.group3 = self._make_group(\n",
    "            in_channels=channel_sizes[1],\n",
    "            out_channels=channel_sizes[2],\n",
    "            depth=group_depths[2],\n",
    "            stride=2,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.group1(x)\n",
    "        x = self.group2(x)\n",
    "        x = self.group3(x)\n",
    "        return x\n",
    "\n",
    "    def _make_group(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        depth: int,\n",
    "        stride: int = 1,\n",
    "    ) -> nn.Sequential:\n",
    "        if stride == 1:\n",
    "            downsample = None\n",
    "        else:\n",
    "            downsample = hnn.HConvolution2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=1,\n",
    "                manifold=self.manifold,\n",
    "                stride=stride,\n",
    "            )\n",
    "\n",
    "        layers = [\n",
    "            PoincareResidualBlock(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                manifold=self.manifold,\n",
    "                stride=stride,\n",
    "                downsample=downsample,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        for _ in range(1, depth):\n",
    "            layers.append(\n",
    "                PoincareResidualBlock(\n",
    "                    in_channels=out_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    manifold=self.manifold,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "class PoincareClassifier(nn.Module):\n",
    "    def __init__(self, in_features: int, num_classes: int, manifold: PoincareBall):\n",
    "        super().__init__()\n",
    "        self.manifold = manifold\n",
    "        self.avg_pool = hnn.HAvgPool2d(kernel_size=8, manifold=manifold)\n",
    "        self.fc = hnn.HLinear(in_features=in_features, out_features=num_classes, manifold=manifold)\n",
    "\n",
    "    def forward(self, x: ManifoldTensor) -> ManifoldTensor:\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.fc(x.squeeze())\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "def AcademicHyperbolicResNet(manifold):\n",
    "    encoder = PoincareEncoder(channel_sizes=[4, 8, 16], group_depths=[3, 3, 3], manifold=manifold)#.to(device)\n",
    "    classifier = PoincareClassifier(in_features=[4, 8, 16][-1], num_classes=10, manifold=manifold)#.to(device)\n",
    "    return encoder, classifier\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HYPBL_ARCHITECTURE == 'complete-ResNet':\n",
    "    if MODEL_NAME == 'ResNet18':\n",
    "        hypbl_encoder, hypbl_classifier = HyperbolicResNet18(manifold=MANIFOLD)\n",
    "    elif MODEL_NAME == 'ResNet34':\n",
    "        hypbl_encoder, hypbl_classifier = HyperbolicResNet34(manifold=MANIFOLD)\n",
    "    elif MODEL_NAME == 'ResNet50':\n",
    "        hypbl_encoder, hypbl_classifier = HyperbolicResNet50(manifold=MANIFOLD)\n",
    "    elif MODEL_NAME == 'ResNet101':\n",
    "        hypbl_encoder, hypbl_classifier = HyperbolicResNet101(manifold=MANIFOLD)\n",
    "    elif MODEL_NAME == 'ResNet152':\n",
    "        hypbl_encoder, hypbl_classifier = HyperbolicResNet152(manifold=MANIFOLD)\n",
    "\n",
    "elif HYPBL_ARCHITECTURE == 'hybrid-ResNet':\n",
    "    if MODEL_NAME == 'ResNet18':\n",
    "        hypbl_encoder, hypbl_classifier = HybridHyperbolicResNet18(manifold=MANIFOLD)\n",
    "    elif MODEL_NAME == 'ResNet34':\n",
    "       hypbl_encoder, hypbl_classifier = HybridHyperbolicResNet34(manifold=MANIFOLD)\n",
    "    elif MODEL_NAME == 'ResNet50':\n",
    "        hypbl_encoder, hypbl_classifier = HybridHyperbolicResNet50(manifold=MANIFOLD)\n",
    "    elif MODEL_NAME == 'ResNet101':\n",
    "        hypbl_encoder, hypbl_classifier = HybridHyperbolicResNet101(manifold=MANIFOLD)\n",
    "    elif MODEL_NAME == 'ResNet152':\n",
    "        hypbl_encoder, hypbl_classifier = HybridHyperbolicResNet152(manifold=MANIFOLD)\n",
    "\n",
    "elif HYPBL_ARCHITECTURE == 'academic-ResNet':\n",
    "    hypbl_encoder, hypbl_classifier = AcademicHyperbolicResNet(manifold=MANIFOLD)\n",
    "\n",
    "\n",
    "if device == 'cuda':\n",
    "    MANIFOLD = MANIFOLD.to(device)\n",
    "    hypbl_encoder = torch.nn.DataParallel(hypbl_encoder).to(device)\n",
    "    hypbl_classifier = torch.nn.DataParallel(hypbl_classifier).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and scheduler definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even in hyperbolic spaces it is not advised to use the *Adam* optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST CONFIGURATION ADAM\n",
    "if OPTIMIZER_NAME == 'adam':\n",
    "    hypbl_optimizer = RiemannianAdam(list(hypbl_encoder.parameters()) + list(hypbl_classifier.parameters()), lr=0.000641388, weight_decay=0.567643)\n",
    "\n",
    "# BEST CONFIGURATION SGD\n",
    "elif OPTIMIZER_NAME == 'sgd':\n",
    "    hypbl_optimizer = RiemannianSGD(list(hypbl_encoder.parameters()) + list(hypbl_classifier.parameters()), lr=0.015254, momentum=0.582486, weight_decay=0.00366885)\n",
    "\n",
    "hypbl_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(hypbl_optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to track the loss and accuracy\n",
    "hypbl_train_loss_history = []\n",
    "hypbl_val_loss_history = []\n",
    "hypbl_train_acc_history = []\n",
    "hypbl_val_acc_history = []\n",
    "\n",
    "hypbl_val_acc = 0 \n",
    "\n",
    "scaler = torch.amp.grad_scaler.GradScaler()\n",
    "\n",
    "early_stopper = EarlyStopper()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "\n",
    "    hypbl_average_train_loss, hypbl_train_acc = train(encoder=hypbl_encoder, \n",
    "                                                      classifier=hypbl_classifier, \n",
    "                                                      loader=trainloader, \n",
    "                                                      criterion=CRITERION, \n",
    "                                                      optimizer=hypbl_optimizer, \n",
    "                                                      scaler=scaler, \n",
    "                                                      hyperbolic=HYPBL_ARCHITECTURE, \n",
    "                                                      manifold=MANIFOLD, \n",
    "                                                      device=device)\n",
    "    \n",
    "    hypbl_train_loss_history.append(hypbl_average_train_loss)\n",
    "    hypbl_train_acc_history.append(hypbl_train_acc)\n",
    "    \n",
    "    hypbl_average_val_loss, hypbl_val_acc = validate(epoch=epoch, \n",
    "                                                     encoder=hypbl_encoder, \n",
    "                                                     classifier=hypbl_classifier, \n",
    "                                                     loader=valloader, \n",
    "                                                     criterion=CRITERION, \n",
    "                                                     best_acc=hypbl_val_acc, \n",
    "                                                     hyperbolic=HYPBL_ARCHITECTURE, \n",
    "                                                     manifold=MANIFOLD, \n",
    "                                                     ckpt_name=HYPBL_ORIGINAL_CKPT_NAME, \n",
    "                                                     device=device)\n",
    "    \n",
    "    hypbl_val_loss_history.append(hypbl_average_val_loss)\n",
    "    hypbl_val_acc_history.append(hypbl_val_acc)\n",
    "\n",
    "    if early_stopper.early_stop(hypbl_average_val_loss):  \n",
    "        print('\\nEarly stopper activated')           \n",
    "        break\n",
    "\n",
    "    hypbl_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss and accuracy over the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(hypbl_train_loss_history, hypbl_val_loss_history, hypbl_train_acc_history, hypbl_val_acc_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load best checkpoint.\n",
    "\n",
    "It is needed since there is no guarantee that the latest epochs produced the most accurate parameters.\n",
    "\n",
    "Also comes in handy for quick testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Resuming from checkpoint...')\n",
    "\n",
    "assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "checkpoint = torch.load('./checkpoint/{}'.format(HYPBL_ORIGINAL_CKPT_NAME))\n",
    "hypbl_encoder.load_state_dict(checkpoint['encoder'])\n",
    "hypbl_classifier.load_state_dict(checkpoint['classifier'])\n",
    "\n",
    "print('Checkpoint loaded!')\n",
    "print(f'\\nValidation average loss: {checkpoint['loss']:.3f}')\n",
    "print(f'Validation set accuracy: {checkpoint['acc']:.3f}%')\n",
    "print(f'From epoch: {checkpoint['epoch']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the model accuracy over some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "hypbl_encoder.eval()\n",
    "hypbl_classifier.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = hypbl_encoder(images)\n",
    "    outputs = hypbl_classifier(features)\n",
    "    \n",
    "    _, predicted = torch.max(outputs.tensor, 1)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images[0:16]))\n",
    "\n",
    "# Prepare rows with GroundTruth and Predicted labels\n",
    "rows = [[\"GroundTruth\", *[classes[labels[j]] for j in range(16)]],\n",
    "        [\"Predicted\", *[classes[predicted[j]] for j in range(16)]]]\n",
    "\n",
    "# Display the table\n",
    "print(tabulate(rows, headers=[\"\"] + [f\"Image {i+1}\" for i in range(16)], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model overall performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypbl_test_loss, hypbl_test_acc = test(encoder=hypbl_encoder, \n",
    "                           classifier=hypbl_classifier, \n",
    "                           loader=testloader, \n",
    "                           criterion=CRITERION, \n",
    "                           hyperbolic='hybrid-ResNet', \n",
    "                           manifold=MANIFOLD, \n",
    "                           device=device)\n",
    "\n",
    "print(f'\\nTest Set - Loss: {hypbl_test_loss:.3f}, Accuracy: {hypbl_test_acc:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose which class samples to display by toggling the class names.\n",
    "\n",
    "**WARNING**: As said in the *Configuration* section, Plotly has some problems with Visual Studio Code. To prevent strange behaviour by the plots, it is advised to delete the cell plot output first if you need to run again the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the test set, and plot them\n",
    "features, labels = extract_features(hypbl_encoder, testloader, hyperbolic=HYPBL_ARCHITECTURE, manifold=MANIFOLD, device=device)\n",
    "create_plot(features, labels, classes, dimension=2, convexhull=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperbolic retrain model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, another instantiation of the model will be trained without the samples that we want to forget (from a single class or with random sampling).\n",
    "\n",
    "Most of the functions are reused from the previous chapters, so refer to them if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HYPBL_ARCHITECTURE == 'complete-ResNet':\n",
    "    if MODEL_NAME == 'ResNet18':\n",
    "        retrain_hypbl_encoder, retrain_hypbl_classifier = HyperbolicResNet18(manifold=MANIFOLD)\n",
    "    elif MODEL_NAME == 'ResNet34':\n",
    "        retrain_hypbl_encoder, retrain_hypbl_classifier = HyperbolicResNet34(manifold=MANIFOLD)\n",
    "    elif MODEL_NAME == 'ResNet50':\n",
    "        retrain_hypbl_encoder, retrain_hypbl_classifier = HyperbolicResNet50(manifold=MANIFOLD)\n",
    "    elif MODEL_NAME == 'ResNet101':\n",
    "        retrain_hypbl_encoder, retrain_hypbl_classifier = HyperbolicResNet101(manifold=MANIFOLD)\n",
    "    elif MODEL_NAME == 'ResNet152':\n",
    "        retrain_hypbl_encoder, retrain_hypbl_classifier = HyperbolicResNet152(manifold=MANIFOLD)\n",
    "\n",
    "elif HYPBL_ARCHITECTURE == 'hybrid-ResNet':\n",
    "    if MODEL_NAME == 'ResNet18':\n",
    "        retrain_hypbl_encoder, retrain_hypbl_classifier = HybridHyperbolicResNet18(manifold=MANIFOLD)\n",
    "    elif MODEL_NAME == 'ResNet34':\n",
    "       retrain_hypbl_encoder, retrain_hypbl_classifier = HybridHyperbolicResNet34(manifold=MANIFOLD)\n",
    "    elif MODEL_NAME == 'ResNet50':\n",
    "        retrain_hypbl_encoder, retrain_hypbl_classifier = HybridHyperbolicResNet50(manifold=MANIFOLD)\n",
    "    elif MODEL_NAME == 'ResNet101':\n",
    "        retrain_hypbl_encoder, retrain_hypbl_classifier = HybridHyperbolicResNet101(manifold=MANIFOLD)\n",
    "    elif MODEL_NAME == 'ResNet152':\n",
    "        retrain_hypbl_encoder, retrain_hypbl_classifier = HybridHyperbolicResNet152(manifold=MANIFOLD)\n",
    "\n",
    "elif HYPBL_ARCHITECTURE == 'academic-ResNet':\n",
    "    retrain_hypbl_encoder, retrain_hypbl_classifier = AcademicHyperbolicResNet(manifold=MANIFOLD)\n",
    "\n",
    "\n",
    "if device == 'cuda':\n",
    "    manifold = MANIFOLD.to(device)\n",
    "    retrain_hypbl_encoder = torch.nn.DataParallel(retrain_hypbl_encoder).to(device)\n",
    "    retrain_hypbl_classifier = torch.nn.DataParallel(retrain_hypbl_classifier).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and scheduler configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST CONFIGURATION ADAM\n",
    "if OPTIMIZER_NAME == 'adam':\n",
    "    retrain_hypbl_optimizer = RiemannianAdam(list(retrain_hypbl_encoder.parameters()) + list(retrain_hypbl_classifier.parameters()), lr=0.000641388, weight_decay=0.567643)\t\n",
    "\n",
    "# BEST CONFIGURATION SGD\n",
    "elif OPTIMIZER_NAME == 'sgd':\n",
    "    retrain_hypbl_optimizer = RiemannianSGD(list(retrain_hypbl_encoder.parameters()) + list(retrain_hypbl_classifier.parameters()), lr=0.015254, momentum=0.582486, weight_decay=0.00366885) \n",
    "    \n",
    "retrain_hypbl_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(retrain_hypbl_optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to track the loss and accuracy\n",
    "retrain_hypbl_train_loss_history = []\n",
    "retrain_hypbl_val_loss_history = []\n",
    "retrain_hypbl_train_acc_history = []\n",
    "retrain_hypbl_val_acc_history = []\n",
    "\n",
    "retrain_hypbl_val_acc = 0 \n",
    "\n",
    "scaler = torch.amp.grad_scaler.GradScaler()\n",
    "\n",
    "early_stopper = EarlyStopper()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "\n",
    "    retrain_hypbl_average_train_loss, retrain_hypbl_train_acc = train(encoder=retrain_hypbl_encoder, \n",
    "                                                                      classifier=retrain_hypbl_classifier, \n",
    "                                                                      loader=remaining_trainloader, \n",
    "                                                                      criterion=CRITERION, \n",
    "                                                                      optimizer=retrain_hypbl_optimizer, \n",
    "                                                                      scaler=scaler, \n",
    "                                                                      hyperbolic=HYPBL_ARCHITECTURE, \n",
    "                                                                      manifold=MANIFOLD, \n",
    "                                                                      device=device)\n",
    "    \n",
    "    retrain_hypbl_train_loss_history.append(retrain_hypbl_average_train_loss)\n",
    "    retrain_hypbl_train_acc_history.append(retrain_hypbl_train_acc)\n",
    "\n",
    "    retrain_hypbl_average_val_loss, retrain_hypbl_val_acc = validate(epoch=epoch, \n",
    "                                                                     encoder=retrain_hypbl_encoder, \n",
    "                                                                     classifier=retrain_hypbl_classifier, \n",
    "                                                                     loader=remaining_valloader, \n",
    "                                                                     criterion=CRITERION, \n",
    "                                                                     best_acc=retrain_hypbl_val_acc, \n",
    "                                                                     hyperbolic=HYPBL_ARCHITECTURE, \n",
    "                                                                     manifold=MANIFOLD, \n",
    "                                                                     ckpt_name=HYPBL_RETRAIN_CKPT_NAME, \n",
    "                                                                     device=device)\n",
    "    \n",
    "    retrain_hypbl_val_loss_history.append(retrain_hypbl_average_val_loss)\n",
    "    retrain_hypbl_val_acc_history.append(retrain_hypbl_val_acc)\n",
    "\n",
    "    if early_stopper.early_stop(retrain_hypbl_average_val_loss):\n",
    "        print('\\nEarly stopper activated') \n",
    "        break\n",
    "\n",
    "    retrain_hypbl_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(retrain_hypbl_train_loss_history, retrain_hypbl_val_loss_history, retrain_hypbl_train_acc_history, retrain_hypbl_val_acc_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load best checkpoint.\n",
    "\n",
    "It is needed since there is no guarantee that the latest epochs produced the most accurate parameters.\n",
    "\n",
    "Also comes in handy for quick testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Resuming from checkpoint...')\n",
    "\n",
    "assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "checkpoint = torch.load('./checkpoint/{}'.format(HYPBL_RETRAIN_CKPT_NAME))\n",
    "retrain_hypbl_encoder.load_state_dict(checkpoint['encoder'])\n",
    "retrain_hypbl_classifier.load_state_dict(checkpoint['classifier'])\n",
    "\n",
    "print('Checkpoint loaded!')\n",
    "print(f'\\nValidation average loss: {checkpoint['loss']:.3f}')\n",
    "print(f'Validation set accuracy: {checkpoint['acc']:.3f}%')\n",
    "print(f'From epoch: {checkpoint['epoch']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the model precision over some samples of the *remaining test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(remaining_testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "retrain_hypbl_encoder.eval()\n",
    "retrain_hypbl_classifier.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = retrain_hypbl_encoder(images)\n",
    "    outputs = retrain_hypbl_classifier(features)\n",
    "\n",
    "    _, predicted = torch.max(outputs.tensor, 1)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images[0:16]))\n",
    "\n",
    "# Prepare rows with GroundTruth and Predicted labels\n",
    "rows = [[\"GroundTruth\", *[classes[labels[j]] for j in range(16)]],\n",
    "        [\"Predicted\", *[classes[predicted[j]] for j in range(16)]]]\n",
    "\n",
    "# Display the table\n",
    "print(tabulate(rows, headers=[\"\"] + [f\"Image {i+1}\" for i in range(16)], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model overall performance on *remaining samples* and *unlearned samples*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_hypbl_average_remaining_test_loss, retrain_hypbl_best_remaining_test_acc = test(encoder=retrain_hypbl_encoder, \n",
    "                                                                                        classifier=retrain_hypbl_classifier, \n",
    "                                                                                        loader=remaining_testloader, \n",
    "                                                                                        criterion=CRITERION, \n",
    "                                                                                        hyperbolic=HYPBL_ARCHITECTURE, \n",
    "                                                                                        manifold=MANIFOLD, \n",
    "                                                                                        device=device)\n",
    "\n",
    "retrain_hypbl_average_unl_train_loss, retrain_hypbl_best_unl_train_acc = test(encoder=retrain_hypbl_encoder, \n",
    "                                                                              classifier=retrain_hypbl_classifier, \n",
    "                                                                              loader=unlearning_trainloader, \n",
    "                                                                              criterion=CRITERION, \n",
    "                                                                              hyperbolic=HYPBL_ARCHITECTURE, \n",
    "                                                                              manifold=MANIFOLD, \n",
    "                                                                              device=device)\n",
    "\n",
    "if SCENARIO == 'single-class':\n",
    "\n",
    "    retrain_hypbl_average_unl_test_loss, retrain_hypbl_best_unl_test_acc = test(encoder=retrain_hypbl_encoder, \n",
    "                                                                                classifier=retrain_hypbl_classifier, \n",
    "                                                                                loader=unlearning_testloader, \n",
    "                                                                                criterion=CRITERION, \n",
    "                                                                                hyperbolic=HYPBL_ARCHITECTURE, \n",
    "                                                                                manifold=MANIFOLD, \n",
    "                                                                                device=device)\n",
    "\n",
    "print(f'\\nRemaining Test Set Loss: {retrain_hypbl_average_remaining_test_loss:.3f}, Accuracy: {retrain_hypbl_best_remaining_test_acc:.3f}%')\n",
    "print(f'Unlearning Train Set Loss: {retrain_hypbl_average_unl_train_loss:.3f}, Accuracy: {retrain_hypbl_best_unl_train_acc:.3f}%')\n",
    "print(f'Unlearning Test Set Loss: {retrain_hypbl_average_unl_test_loss:.3f}, Accuracy: {retrain_hypbl_best_unl_test_acc:.3f}%') if SCENARIO == 'single-class' else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose which class samples to display by toggling the class names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCENARIO == 'single-class':\n",
    "\n",
    "    # Extract features from the test set\n",
    "    remaining_features, remaining_labels = extract_features(retrain_hypbl_encoder, remaining_testloader, hyperbolic=HYPBL_ARCHITECTURE, manifold=MANIFOLD, device=device)\n",
    "    unlearning_features, unlearning_labels = extract_features(retrain_hypbl_encoder, unlearning_testloader, hyperbolic=HYPBL_ARCHITECTURE, manifold=MANIFOLD, device=device)\n",
    "\n",
    "    # Combine for t-SNE\n",
    "    features = np.concatenate((unlearning_features, remaining_features), axis=0)\n",
    "    labels = np.concatenate((unlearning_labels, remaining_labels), axis=0)\n",
    "\n",
    "# In this case, there is no specific class to isolate, so the plotting goal is for all the models (original, retrain and unlearned) to plot a similar distribution of the whole test set\n",
    "elif SCENARIO == 'random-sample':\n",
    "\n",
    "    features, labels = extract_features(retrain_hypbl_encoder, remaining_testloader, hyperbolic=HYPBL_ARCHITECTURE, manifold=MANIFOLD, device=device)\n",
    "    \n",
    "\n",
    "create_plot(features, labels, classes, dimension=2, convexhull=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlearning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section i will replicate the unlearning procedure but on the new representation space.\n",
    "\n",
    "You will find details about it on the next cells "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: the encoder and classifier here are the original ones (instantiated in the first part of the *Chapter 2*).\n",
    "\n",
    "During the unlearning procedure, the original model parameters will be overwritten.\n",
    "\n",
    "So if you go back and try inference on the original model, the output will change (for worse, obviously).\n",
    "\n",
    "*This checkpoint cell is just for quick testing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Resuming from checkpoint...')\n",
    "\n",
    "assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "checkpoint = torch.load('./checkpoint/{}'.format(HYPBL_ORIGINAL_CKPT_NAME))\n",
    "hypbl_encoder.load_state_dict(checkpoint['encoder'])\n",
    "hypbl_classifier.load_state_dict(checkpoint['classifier'])\n",
    "\n",
    "print('Checkpoint loaded!')\n",
    "print(f'\\nValidation average loss: {checkpoint['loss']:.3f}')\n",
    "print(f'Validation set accuracy: {checkpoint['acc']:.3f}%')\n",
    "print(f'From epoch: {checkpoint['epoch']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic contrastive unlearning loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the ***Constrastive Unlearning*** loss variant to work with the Poincar ball manifold.\n",
    "\n",
    "The code is essentially the same, but it changes how the distances between samples are computed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperbolicContrastiveUnlearningLoss(nn.Module):\n",
    "    def __init__(self, temperature, manifold):\n",
    "        super(HyperbolicContrastiveUnlearningLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.manifold = manifold\n",
    "\n",
    "    def forward(self, embeddings_u, embeddings_r, labels_u, labels_r):\n",
    "        unlearning_batch_size = embeddings_u.size(0)\n",
    "        loss = 0.0\n",
    "\n",
    "        # Normalize and project embeddings in hyperbolic space\n",
    "        embeddings_u = F.normalize(embeddings_u.tensor, p=2, dim=1)\n",
    "        projection_embeddings_u = TangentTensor(data=embeddings_u, man_dim=1, manifold=self.manifold)\n",
    "        embeddings_u = self.manifold.expmap(projection_embeddings_u)\n",
    "\n",
    "        embeddings_r = F.normalize(embeddings_r.tensor, p=2, dim=1)\n",
    "        projection_embeddings_r = TangentTensor(data=embeddings_r, man_dim=1, manifold=self.manifold)\n",
    "        embeddings_r = self.manifold.expmap(projection_embeddings_r)\n",
    "        \n",
    "\n",
    "        for i in range(unlearning_batch_size):\n",
    "            # Anchor: embedding of unlearning sample\n",
    "            z_u = embeddings_u[i]\n",
    "\n",
    "            if SCENARIO == 'single-class':\n",
    "                negatives = embeddings_r\n",
    "                neg_sim = torch.exp(self.manifold.dist(z_u, negatives) / self.temperature)\n",
    "\n",
    "                denominator = neg_sim.size(0) + 1e-8\n",
    "\n",
    "                inner_loss = torch.log(neg_sim / denominator).sum() if len(neg_sim) > 0 else torch.tensor(0.0)\n",
    "                loss += (-1 / (neg_sim.size(0) + 1e-8)) * inner_loss\n",
    "\n",
    "                if len(neg_sim) == 0:\n",
    "                    print(\"Warning: neg_sim is zero or too small.\")\n",
    "\n",
    "            elif SCENARIO == 'random-sample':\n",
    "                pos_mask = labels_r == labels_u[i]\n",
    "                neg_mask = ~pos_mask\n",
    "\n",
    "                positives = embeddings_r[pos_mask]\n",
    "                negatives = embeddings_r[neg_mask]\n",
    "\n",
    "                #print(positives.dim())\n",
    "\n",
    "                pos_sim = torch.exp(self.manifold.dist(z_u, positives) / self.temperature) if len(positives.tensor) > 0 else torch.tensor(0.0)\n",
    "                neg_sim = torch.exp(self.manifold.dist(z_u, negatives) / self.temperature)\n",
    "\n",
    "                denominator = pos_sim.sum() + 1e-8\n",
    "\n",
    "                inner_loss = torch.log(neg_sim / denominator).sum() if len(neg_sim) > 0 else torch.tensor(0.0)\n",
    "                loss += (-1 / (neg_sim.size(0) + 1e-8)) * inner_loss\n",
    "\n",
    "        return loss / unlearning_batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO NOT UNCOMMENT THESE: Hyperbolic unlearning hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Put large objects in Ray's object store for efficient memory management\n",
    "large_unlearning_trainloader = ray.put(unlearning_trainloader)\n",
    "if SCENARIO == 'random-sample':\n",
    "    large_unl_valloader_train = ray.put(unl_valloader_train)\n",
    "large_unl_valloader_test = ray.put(unl_valloader_test)\n",
    "large_remaining_trainset = ray.put(remaining_trainset)\n",
    "\n",
    "checkpoint = torch.load('./checkpoint/{}'.format(HYPBL_ORIGINAL_CKPT_NAME))\n",
    "large_checkpoint = ray.put(checkpoint)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Main training loop with validation\n",
    "def tune_train_contrastive_unlearning(config):\n",
    "\n",
    "    # DA TOGLIERE INSIEME A DATA PARALLEL\n",
    "    def remove_module_prefix(state_dict):\n",
    "        return {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "    \n",
    "    scaler = torch.amp.grad_scaler.GradScaler()\n",
    "\n",
    "    hyperbolic = HYPBL_ARCHITECTURE\n",
    "\n",
    "    curvature = Curvature(value=-1.0)\n",
    "    manifold = PoincareBall(c=curvature).to(device)\n",
    "\n",
    "    encoder, classifier = HybridHyperbolicResNet34(manifold=manifold)\n",
    "\n",
    "    checkpoint = ray.get(large_checkpoint)\n",
    "\n",
    "    encoder_state = remove_module_prefix(checkpoint['encoder'])\n",
    "    classifier_state = remove_module_prefix(checkpoint['classifier'])\n",
    "\n",
    "    encoder.load_state_dict(encoder_state)\n",
    "    classifier.load_state_dict(classifier_state)\n",
    "\n",
    "    encoder = encoder.to(device)\n",
    "    classifier = classifier.to(device)\n",
    "\n",
    "    unlearning_trainloader = ray.get(large_unlearning_trainloader)\n",
    "    remaining_trainset = ray.get(large_remaining_trainset)\n",
    "    if SCENARIO == 'random-sample':\n",
    "        unl_valloader_train = ray.get(large_unl_valloader_train)\n",
    "    unl_valloader_test = ray.get(large_unl_valloader_test)\n",
    "    \n",
    "    criterion = CRITERION\n",
    "\n",
    "    if config['opt'] == 'sgd':\n",
    "        optimizer = RiemannianSGD(list(encoder.parameters()) + list(classifier.parameters()), lr=0.015254, momentum=0.582486, weight_decay=0.00366885)\n",
    "    elif config['opt'] == 'adam':\n",
    "        optimizer = RiemannianAdam(list(encoder.parameters()) + list(classifier.parameters()), lr=0.000641388, weight_decay=0.567643)\n",
    "    \n",
    "    class_count = 10\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "\n",
    "        encoder.train()\n",
    "        classifier.train()\n",
    "\n",
    "        total_loss_epoch = 0.0  # To accumulate loss over the epoch\n",
    "        total_batches = 0\n",
    "        \n",
    "        # Iterate over unlearning dataset (D_u)\n",
    "        for x_u, y_u in unlearning_trainloader:\n",
    "            x_u, y_u = x_u.to(device), y_u.to(device)\n",
    "\n",
    "            # The projection is needed ony if the whole model is hyperbolic\n",
    "            if hyperbolic in ['complete-ResNet', 'academic-ResNet']:\n",
    "                # move the inputs to the manifold\n",
    "                tangents = TangentTensor(data=x_u, man_dim=1, manifold=manifold)\n",
    "                x_u = manifold.expmap(tangents)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss = 0.0  # To accumulate loss over the batch\n",
    "            \n",
    "            # Perform  iterations over the remaining dataset for each unlearning batch\n",
    "            for _ in range(config[\"omega\"]):  # Omega loop\n",
    "\n",
    "                # Create a DataLoader with a RandomSampler\n",
    "                random_sampler = torch.utils.data.RandomSampler(remaining_trainset)\n",
    "                remaining_trainset_loader = torch.utils.data.DataLoader(remaining_trainset, sampler=random_sampler, batch_size=128, pin_memory=True)\n",
    "\n",
    "                for remaining_data in remaining_trainset_loader:\n",
    "                    x_r, y_r = remaining_data\n",
    "                    break\n",
    "\n",
    "                x_r, y_r = x_r.to(device), y_r.to(device)\n",
    "\n",
    "                # The projection is needed ony if the whole model is hyperbolic\n",
    "                if hyperbolic in ['complete-ResNet', 'academic-ResNet']:\n",
    "                    # move the inputs to the manifold\n",
    "                    tangents = TangentTensor(data=x_r, man_dim=1, manifold=manifold)\n",
    "                    x_r = manifold.expmap(tangents)\n",
    "\n",
    "                # Use mixed precision\n",
    "                with torch.amp.autocast_mode.autocast(device_type=device, dtype=torch.float16):\n",
    "                    # Forward pass for remaining samples (classification loss)\n",
    "                    z_r = encoder(x_r) \n",
    "                    logits_r = classifier(z_r)\n",
    "\n",
    "                    # Forward pass for unlearning samples\n",
    "                    z_u = encoder(x_u)\n",
    "                    logits_u = classifier(z_u)\n",
    "\n",
    "                # Compute classification loss\n",
    "                # Indipendently of the hyperbolic model, we need to extract the tensor  \n",
    "                if hyperbolic in ['complete-ResNet', 'hybrid-ResNet', 'academic-ResNet']:\n",
    "                    # Needed for the loss computation\n",
    "                    logits_r = logits_r.tensor # This gives the underlying PyTorch tensor\n",
    "\n",
    "                ce_loss = criterion(logits_r, y_r)\n",
    "                # Compute contrastive unlearning loss\n",
    "                loss_fn = HyperbolicContrastiveUnlearningLoss(temperature=config[\"temperature\"], manifold=manifold)\n",
    "                ul_loss = loss_fn(z_u, z_r, y_u, y_r)\n",
    "\n",
    "                # Total loss: classification loss + contrastive unlearning loss\n",
    "                total_loss = config[\"regularizer_ul\"] * ul_loss + config[\"regularizer_ce\"] * ce_loss\n",
    "                batch_loss += total_loss.item()\n",
    "\n",
    "                scaler.scale(total_loss).backward()\n",
    "                \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_batches += 1\n",
    "            total_loss_epoch += batch_loss    \n",
    "            \n",
    "        # Compute validation for termination criteria\n",
    "        if SCENARIO == 'single-class':\n",
    "            unlearned_acc, verdict = validate_unlearning(encoder, classifier, None, unl_valloader_test, CRITERION, class_count=class_count, hyperbolic=hyperbolic, manifold=manifold)\n",
    "            # Compute accuracy retain over the remaining samples\n",
    "            remaining_acc = validate_containment(encoder, classifier, remaining_testloader, CRITERION, class_count=class_count, hyperbolic=hyperbolic, manifold=manifold)\n",
    "        elif SCENARIO == 'random-sample':\n",
    "            unlearned_acc, verdict = validate_unlearning(encoder, classifier, unl_valloader_train, unl_valloader_test, CRITERION, class_count=class_count, hyperbolic=hyperbolic, manifold=manifold)\n",
    "\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # report results to Ray Tune\n",
    "        ray.train.report(dict(unlearned_accuracy=unlearned_acc, remaining_accuracy=remaining_acc))       \n",
    "\n",
    "        # Termination criteria\n",
    "        if verdict: \n",
    "            print(\"\\nTerminating unlearning procedure\")\n",
    "            break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def custom_trial_name(trial):\n",
    "    return f\"trial_unlearning{trial.trial_id}\"\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "search_space = {\n",
    "    \"num_epochs\": tune.grid_search([NUM_EPOCHS]),\n",
    "    \"opt\": tune.grid_search(['sgd']),\n",
    "    \"temperature\": tune.uniform(0.1, 1.0),\n",
    "    \"regularizer_ce\": tune.uniform(0.1, 1.0),\n",
    "    \"regularizer_ul\": tune.uniform(0.1, 1.0),\n",
    "    \"omega\": tune.choice([2, 4, 6]),\n",
    "}\n",
    "\n",
    "# Use partial to pass fixed_params\n",
    "analysis = tune.run(\n",
    "    tune_train_contrastive_unlearning,\n",
    "    resources_per_trial={\"cpu\": 20, \"gpu\": 1},\n",
    "    config=search_space,\n",
    "    num_samples=50,\n",
    "    scheduler=ASHAScheduler(metric=\"remaining_accuracy\", mode=\"max\"),    \n",
    "    trial_dirname_creator=custom_trial_name\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unlearning optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we assume that the training hyperparamters of the Euclidean model are also good for unlearning in Hyperbolic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST CONFIGURATION ADAM\n",
    "if OPTIMIZER_NAME == 'adam':\n",
    "    unl_hypbl_optimizer = RiemannianAdam(list(hypbl_encoder.parameters()) + list(hypbl_classifier.parameters()), lr=0.000641388, weight_decay=0.567643)\t\n",
    "\n",
    "# BEST CONFIGURATION SGD\n",
    "elif OPTIMIZER_NAME == 'sgd':\n",
    "    unl_hypbl_optimizer = RiemannianSGD(list(hypbl_encoder.parameters()) + list(hypbl_classifier.parameters()), lr=0.015254, momentum=0.582486, weight_decay=0.00366885) \n",
    "\n",
    "unl_hypbl_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(unl_hypbl_optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.amp.grad_scaler.GradScaler()\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "\n",
    "    train_contrastive_unlearning(encoder=hypbl_encoder, \n",
    "                                 classifier=hypbl_classifier, \n",
    "                                 unlearning_trainloader=unlearning_trainloader, \n",
    "                                 remaining_trainset=remaining_trainset, \n",
    "                                 classification_loss=CRITERION, \n",
    "                                 unlearning_loss=HyperbolicContrastiveUnlearningLoss(temperature=HYPBL_TEMPERATURE, manifold=MANIFOLD),\n",
    "                                 optimizer=unl_hypbl_optimizer, \n",
    "                                 omega=HYPBL_OMEGA, \n",
    "                                 lambda_ce=HYPBL_REGULARIZER_CE, \n",
    "                                 lambda_ul=HYPBL_REGULARIZER_UL,  \n",
    "                                 scaler=scaler,\n",
    "                                 hyperbolic=HYPBL_ARCHITECTURE,\n",
    "                                 manifold=MANIFOLD,\n",
    "                                 device=device\n",
    "                                 )\n",
    "\n",
    "    if SCENARIO == 'single-class':\n",
    "        unlearned_hypbl_acc, verdict = validate_unlearning(encoder=hypbl_encoder, \n",
    "                                                           classifier=hypbl_classifier, \n",
    "                                                           valloader_train=None, \n",
    "                                                           valloader_test=unl_valloader_test, \n",
    "                                                           classification_loss=CRITERION, \n",
    "                                                           class_count=len(classes),\n",
    "                                                           hyperbolic=HYPBL_ARCHITECTURE,\n",
    "                                                           manifold=MANIFOLD,\n",
    "                                                           device=device\n",
    "                                                           )\n",
    "        \n",
    "        remaining_hypbl_acc = validate_containment(encoder=hypbl_encoder, \n",
    "                                                   classifier=hypbl_classifier, \n",
    "                                                   remaining_testloader=remaining_testloader, \n",
    "                                                   classification_loss=CRITERION, \n",
    "                                                   class_count=len(classes), \n",
    "                                                   hyperbolic=HYPBL_ARCHITECTURE, \n",
    "                                                   manifold=MANIFOLD, \n",
    "                                                   device=device\n",
    "                                                   )\n",
    "\n",
    "    elif SCENARIO == 'random-sample':\n",
    "        unlearned_hypbl_acc, verdict = validate_unlearning(encoder=hypbl_encoder, \n",
    "                                                           classifier=hypbl_classifier, \n",
    "                                                           valloader_train=unl_valloader_train, \n",
    "                                                           valloader_test=unl_valloader_test, \n",
    "                                                           classification_loss=CRITERION, \n",
    "                                                           class_count=len(classes), \n",
    "                                                           hyperbolic=HYPBL_ARCHITECTURE,\n",
    "                                                           manifold=MANIFOLD,\n",
    "                                                           device=device\n",
    "                                                           )\n",
    "    \n",
    "    print(f\"\\nTermination condition reached: {verdict}\")\n",
    "\n",
    "    # Termination criteria\n",
    "    if verdict: \n",
    "        print(\"\\nTerminating unlearning procedure\")\n",
    "        break\n",
    "\n",
    "    unl_hypbl_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of unlearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unlearning efficiency is tested on the same subsets used by the authors for the experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *single-class* scenario --> remaining test set, unlearning train set, unleraning test set\n",
    "- *random-sample* scenario --> remaining test set, unlearning train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlearned_hypbl_average_remaining_test_loss, unlearned_hypbl_best_remaining_test_acc = test(encoder=hypbl_encoder, \n",
    "                                                                                            classifier=hypbl_classifier, \n",
    "                                                                                            loader=remaining_testloader, \n",
    "                                                                                            criterion=CRITERION, \n",
    "                                                                                            hyperbolic=HYPBL_ARCHITECTURE, \n",
    "                                                                                            manifold=MANIFOLD, \n",
    "                                                                                            device=device)\n",
    "\n",
    "unlearned_hypbl_average_unl_train_loss, unlearned_hypbl_best_unl_train_acc = test(encoder=hypbl_encoder, \n",
    "                                                                                    classifier=hypbl_classifier, \n",
    "                                                                                    loader=unlearning_trainloader,\n",
    "                                                                                    criterion=CRITERION,\n",
    "                                                                                    hyperbolic=HYPBL_ARCHITECTURE,\n",
    "                                                                                    manifold=MANIFOLD,\n",
    "                                                                                    device=device)\n",
    "\n",
    "if SCENARIO == 'single-class':\n",
    "\n",
    "    unlearned_hypbl_average_unl_test_loss, unlearned_hypbl_best_unl_test_acc = test(encoder=hypbl_encoder,\n",
    "                                                                                    classifier=hypbl_classifier,\n",
    "                                                                                    loader=unlearning_testloader,\n",
    "                                                                                    criterion=CRITERION,\n",
    "                                                                                    hyperbolic=HYPBL_ARCHITECTURE,\n",
    "                                                                                    manifold=MANIFOLD,\n",
    "                                                                                    device=device)\n",
    "\n",
    "print(f'\\nUnlearned Remaining Test Set Loss: {unlearned_hypbl_average_remaining_test_loss:.3f}, Accuracy: {unlearned_hypbl_best_remaining_test_acc:.3f}%')\n",
    "print(f'Unlearned Unlearning Train Set Loss: {unlearned_hypbl_average_unl_train_loss:.3f}, Accuracy: {unlearned_hypbl_best_unl_train_acc:.3f}%')\n",
    "print(f'Unlearned Unlearning Test Set Loss: {unlearned_hypbl_average_unl_test_loss:.3f}, Accuracy: {unlearned_hypbl_best_unl_test_acc:.3f}%') if SCENARIO == 'single-class' else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions over batch sample of the *remaining test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(remaining_testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "hypbl_encoder.eval()\n",
    "hypbl_classifier.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = hypbl_encoder(images)\n",
    "    outputs = hypbl_classifier(features)\n",
    "\n",
    "    _, predicted = torch.max(outputs.tensor, 1)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images[0:16]))\n",
    "\n",
    "# Prepare rows with GroundTruth and Predicted labels\n",
    "rows = [[\"GroundTruth\", *[classes[labels[j]] for j in range(16)]],\n",
    "        [\"Predicted\", *[classes[predicted[j]] for j in range(16)]]]\n",
    "\n",
    "# Display the table\n",
    "print(tabulate(rows, headers=[\"\"] + [f\"Image {i+1}\" for i in range(16)], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions over batch sample of the *unlearned set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCENARIO == 'single-class':\n",
    "    \n",
    "    dataiter = iter(unlearning_testloader)\n",
    "    images, labels = next(dataiter)\n",
    "\n",
    "elif SCENARIO == 'random-sample':\n",
    "\n",
    "    dataiter = iter(unlearning_trainloader)\n",
    "    images, labels = next(dataiter)\n",
    "    \n",
    "\n",
    "hypbl_encoder.eval()\n",
    "hypbl_classifier.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    features = hypbl_encoder(images)\n",
    "    outputs = hypbl_classifier(features)\n",
    "\n",
    "    _, predicted = torch.max(outputs.tensor, 1)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images[0:16]))\n",
    "\n",
    "# Prepare rows with GroundTruth and Predicted labels\n",
    "rows = [[\"GroundTruth\", *[classes[labels[j]] for j in range(16)]],\n",
    "        [\"Predicted\", *[classes[predicted[j]] for j in range(16)]]]\n",
    "\n",
    "# Display the table\n",
    "print(tabulate(rows, headers=[\"\"] + [f\"Image {i+1}\" for i in range(16)], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCENARIO == 'single-class':\n",
    "\n",
    "    # Extract features from the test set\n",
    "    remaining_features, remaining_labels = extract_features(hypbl_encoder, remaining_testloader, hyperbolic=HYPBL_ARCHITECTURE, manifold=MANIFOLD, device=device)\n",
    "    unlearned_features, unlearned_labels = extract_features(hypbl_encoder, unlearning_testloader, hyperbolic=HYPBL_ARCHITECTURE, manifold=MANIFOLD, device=device)\n",
    "\n",
    "    # Combine for t-SNE\n",
    "    features = np.concatenate((unlearned_features, remaining_features), axis=0)\n",
    "    labels = np.concatenate((unlearned_labels, remaining_labels), axis=0)\n",
    "\n",
    "# In this case, there is no specific class to isolate, so the plotting goal is for all the models (original, retrain and unlearned) to plot a similar distribution of the whole test set\n",
    "elif SCENARIO == 'random-sample':\n",
    "\n",
    "    features, labels = extract_features(hypbl_encoder, remaining_testloader, hyperbolic=HYPBL_ARCHITECTURE, manifold=MANIFOLD, device=device)\n",
    "\n",
    "create_plot(features, labels, classes, dimension=2, convexhull=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
